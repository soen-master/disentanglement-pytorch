name=slurm_script
Pre-Processing CelebA
labels (30000, 32)
All classes percentages: [0.2936666666666667, 0.2319, 0.12203333333333333, 0.18716666666666668, 0.16523333333333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Initialized GrayVAE_Join model
## Initializing Train indexes
->path chosen:: logs/celebA__GrayVAE_Join__2022_04/28-09-39
Losses: {'total_vae': tensor(15938.7324, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(8573.9541, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(0.6669, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1660.0967, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(5704.0151, device='cuda:0', grad_fn=<MulBackward0>)}
[0:239]  loss_total_vae=13293.662  loss_recon=7828.591  loss_kld=406.937  loss_prediction=1520.227  loss_true_values=3537.907  loss_total_vae_epoch=13912.624  
tracking changes
Saved model at epoch 1
Losses: {'total_vae': tensor(13519.0684, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(7906.6172, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(407.1962, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1588.6492, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(3616.6052, device='cuda:0', grad_fn=<MulBackward0>)}
[1:479]  loss_total_vae=12497.988  loss_recon=7196.851  loss_kld=517.211  loss_prediction=1472.198  loss_true_values=3311.727  loss_total_vae_epoch=12917.227  
tracking changes
Saved model at epoch 2
Losses: {'total_vae': tensor(12550.5303, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(7249.8159, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(560.5510, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1507.8053, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(3232.3572, device='cuda:0', grad_fn=<MulBackward0>)}
[2:719]  loss_total_vae=12287.980  loss_recon=7092.124  loss_kld=590.776  loss_prediction=1470.625  loss_true_values=3134.456  loss_total_vae_epoch=12428.598  
tracking changes
Saved model at epoch 3
Losses: {'total_vae': tensor(12218.7100, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(7053.2544, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(606.1102, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1487.5347, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(3071.8105, device='cuda:0', grad_fn=<MulBackward0>)}
[3:959]  loss_total_vae=12163.191  loss_recon=7019.521  loss_kld=608.677  loss_prediction=1409.511  loss_true_values=3125.482  loss_total_vae_epoch=12232.834  
tracking changes
Saved model at epoch 4
Losses: {'total_vae': tensor(12148.6436, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(7142.6729, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(612.2045, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1357.3479, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(3036.4180, device='cuda:0', grad_fn=<MulBackward0>)}
[4:1199]  loss_total_vae=12101.610  loss_recon=6981.505  loss_kld=603.510  loss_prediction=1320.846  loss_true_values=3195.748  loss_total_vae_epoch=12036.395  
tracking changes
Saved model at epoch 5
Losses: {'total_vae': tensor(11954.5625, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6993.3784, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(625.7307, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1256.1023, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(3079.3508, device='cuda:0', grad_fn=<MulBackward0>)}
[5:1439]  loss_total_vae=11608.528  loss_recon=6903.434  loss_kld=665.727  loss_prediction=1125.789  loss_true_values=2913.579  loss_total_vae_epoch=11848.461  
tracking changes
Saved model at epoch 6
Losses: {'total_vae': tensor(11638.3535, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6942.9800, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(667.7695, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1132.2074, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2895.3965, device='cuda:0', grad_fn=<MulBackward0>)}
[6:1679]  loss_total_vae=11564.400  loss_recon=6922.981  loss_kld=704.845  loss_prediction=1075.440  loss_true_values=2861.135  loss_total_vae_epoch=11701.545  
tracking changes
Saved model at epoch 7
Losses: {'total_vae': tensor(11574.5234, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6873.7661, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(723.6994, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1022.9711, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2954.0869, device='cuda:0', grad_fn=<MulBackward0>)}
[7:1919]  loss_total_vae=11325.039  loss_recon=6785.736  loss_kld=694.817  loss_prediction=1035.140  loss_true_values=2809.346  loss_total_vae_epoch=11593.078  
tracking changes
Saved model at epoch 8
Losses: {'total_vae': tensor(11668.9883, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6947.4736, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(721.2820, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1127.7124, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2872.5195, device='cuda:0', grad_fn=<MulBackward0>)}
[8:2159]  loss_total_vae=11586.424  loss_recon=6941.755  loss_kld=695.790  loss_prediction=1155.467  loss_true_values=2793.411  loss_total_vae_epoch=11520.119  
tracking changes
Saved model at epoch 9
Losses: {'total_vae': tensor(11560.7461, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(7059.0879, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(694.2716, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1049.4498, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2757.9365, device='cuda:0', grad_fn=<MulBackward0>)}
[9:2399]  loss_total_vae=11504.312  loss_recon=6886.367  loss_kld=676.696  loss_prediction=1039.221  loss_true_values=2902.029  loss_total_vae_epoch=11460.035  
tracking changes
Saved model at epoch 10
Losses: {'total_vae': tensor(11350.1484, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6841.9165, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(705.4957, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(961.7614, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2840.9751, device='cuda:0', grad_fn=<MulBackward0>)}
[10:2639]  loss_total_vae=11229.168  loss_recon=6715.032  loss_kld=707.305  loss_prediction=971.971  loss_true_values=2834.860  loss_total_vae_epoch=11413.151  
tracking changes
Validation stop evaluation
2639 10
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
Saved model at epoch 11
Losses: {'total_vae': tensor(11330.2080, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6770.0054, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(684.4382, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1145.9054, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2729.8591, device='cuda:0', grad_fn=<MulBackward0>)}
[11:2879]  loss_total_vae=11368.614  loss_recon=6914.116  loss_kld=700.716  loss_prediction=982.298  loss_true_values=2771.483  loss_total_vae_epoch=11361.478  
tracking changes
Validation stop evaluation
2879 11
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
Saved model at epoch 12
Losses: {'total_vae': tensor(11349.6172, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6850.9653, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(705.2085, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(1013.8099, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2779.6338, device='cuda:0', grad_fn=<MulBackward0>)}
[12:3119]  loss_total_vae=11223.979  loss_recon=6750.062  loss_kld=747.568  loss_prediction=952.522  loss_true_values=2773.828  loss_total_vae_epoch=11322.425  
tracking changes
Validation stop evaluation
3119 12
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
Saved model at epoch 13
Losses: {'total_vae': tensor(11314.3447, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6804.6362, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(765.9767, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(926.4671, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2817.2646, device='cuda:0', grad_fn=<MulBackward0>)}
[13:3359]  loss_total_vae=11467.329  loss_recon=6797.294  loss_kld=753.919  loss_prediction=1158.854  loss_true_values=2757.260  loss_total_vae_epoch=11284.583  
tracking changes
Validation stop evaluation
3359 13
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
Now val counter at: 1
Latent 0.3938148101170858   0.4028677761554718
BCE 0.9435945967833201 0.9978500525156657
Saved model at epoch 14
Losses: {'total_vae': tensor(11237.6289, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6848.0762, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(707.5113, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(930.2858, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2751.7554, device='cuda:0', grad_fn=<MulBackward0>)}
[14:3599]  loss_total_vae=11421.659  loss_recon=6907.643  loss_kld=781.033  loss_prediction=1041.900  loss_true_values=2691.083  loss_total_vae_epoch=11250.827  
tracking changes
Validation stop evaluation
3599 14
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
Saved model at epoch 15
Losses: {'total_vae': tensor(11054.4580, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6676.4932, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(764.9907, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(921.0065, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2691.9680, device='cuda:0', grad_fn=<MulBackward0>)}
[15:3839]  loss_total_vae=11248.404  loss_recon=6856.217  loss_kld=725.632  loss_prediction=942.821  loss_true_values=2723.734  loss_total_vae_epoch=11220.631  
tracking changes
Validation stop evaluation
3839 15
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
Now val counter at: 1
Latent 0.39182488520940145   0.39572091036372714
BCE 0.9314248919486999 0.964774023161994
Saved model at epoch 16
Losses: {'total_vae': tensor(11265.8496, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6813.6943, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(717.6514, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(977.7279, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2756.7766, device='cuda:0', grad_fn=<MulBackward0>)}
[16:4079]  loss_total_vae=11118.486  loss_recon=6813.992  loss_kld=745.342  loss_prediction=823.945  loss_true_values=2735.207  loss_total_vae_epoch=11197.138  
tracking changes
Validation stop evaluation
4079 16
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
Saved model at epoch 17
Losses: {'total_vae': tensor(11150.9805, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6759.2510, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(787.9034, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(891.9064, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2711.9199, device='cuda:0', grad_fn=<MulBackward0>)}
[17:4319]  loss_total_vae=11215.892  loss_recon=6784.514  loss_kld=702.304  loss_prediction=943.782  loss_true_values=2785.292  loss_total_vae_epoch=11164.317  
tracking changes
Validation stop evaluation
4319 17
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
Now val counter at: 1
Latent 0.3909797509511312   0.3917821364270317
BCE 0.918215827147166 0.9370413614643945
Saved model at epoch 18
Losses: {'total_vae': tensor(11283.5898, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6918.6035, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(725.5866, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(902.6576, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2736.7427, device='cuda:0', grad_fn=<MulBackward0>)}
[18:4559]  loss_total_vae=11269.578  loss_recon=6976.851  loss_kld=710.914  loss_prediction=958.862  loss_true_values=2622.952  loss_total_vae_epoch=11146.206  
tracking changes
Validation stop evaluation
4559 18
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
Saved model at epoch 19
Losses: {'total_vae': tensor(11045.7998, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6757.7656, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(752.5605, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(902.2621, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2633.2119, device='cuda:0', grad_fn=<MulBackward0>)}
[19:4799]  loss_total_vae=11159.786  loss_recon=6790.930  loss_kld=765.376  loss_prediction=851.105  loss_true_values=2752.376  loss_total_vae_epoch=11120.071  
tracking changes
Validation stop evaluation
4799 19
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
Now val counter at: 1
Latent 0.3851110488176346   0.39019721349080405
BCE 0.9033862213293712 0.9227987382147047
Saved model at epoch 20
Losses: {'total_vae': tensor(11198.6562, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6880.2974, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(746.0204, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(878.9464, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2693.3918, device='cuda:0', grad_fn=<MulBackward0>)}
[20:5039]  loss_total_vae=11186.813  loss_recon=6882.087  loss_kld=761.668  loss_prediction=896.003  loss_true_values=2647.057  loss_total_vae_epoch=11105.668  
tracking changes
Validation stop evaluation
5039 20
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
Saved model at epoch 21
Losses: {'total_vae': tensor(11018.3555, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6757.3516, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(748.7523, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(845.2261, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2667.0256, device='cuda:0', grad_fn=<MulBackward0>)}
[21:5279]  loss_total_vae=11152.131  loss_recon=6739.215  loss_kld=743.068  loss_prediction=1047.860  loss_true_values=2621.989  loss_total_vae_epoch=11082.558  
tracking changes
Validation stop evaluation
5279 21
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
Now val counter at: 1
Latent 0.38774582246939343   0.3868460638655557
BCE 0.8934560080369314 0.9066033575269912
Saved model at epoch 22
Losses: {'total_vae': tensor(11159.0947, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6906.5610, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(696.7090, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(876.4807, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2679.3435, device='cuda:0', grad_fn=<MulBackward0>)}
[22:5519]  loss_total_vae=11159.504  loss_recon=6926.363  loss_kld=828.945  loss_prediction=841.628  loss_true_values=2562.568  loss_total_vae_epoch=11061.189  
tracking changes
Validation stop evaluation
5519 22
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
Saved model at epoch 23
Losses: {'total_vae': tensor(11057.9561, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6800.6987, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(767.3447, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(803.3422, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2686.5701, device='cuda:0', grad_fn=<MulBackward0>)}
[23:5759]  loss_total_vae=11194.748  loss_recon=6827.896  loss_kld=739.858  loss_prediction=914.174  loss_true_values=2712.821  loss_total_vae_epoch=11042.455  
tracking changes
Validation stop evaluation
5759 23
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
Now val counter at: 1
Latent 0.3826411575078964   0.38575574325190654
BCE 0.8840018729368846 0.8955725292364756
Saved model at epoch 24
Losses: {'total_vae': tensor(11065.4668, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6748.1792, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(730.7113, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(902.8804, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2683.6956, device='cuda:0', grad_fn=<MulBackward0>)}
[24:5999]  loss_total_vae=11072.833  loss_recon=6804.062  loss_kld=733.045  loss_prediction=845.022  loss_true_values=2690.704  loss_total_vae_epoch=11028.468  
tracking changes
Validation stop evaluation
5999 24
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
Now val counter at: 1
Latent 0.3849481463432312   0.3844258666038513
BCE 0.8866871158281963 0.8890099452601539
Saved model at epoch 25
Losses: {'total_vae': tensor(10835.3086, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6678.3486, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(735.5689, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(802.7109, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2618.6802, device='cuda:0', grad_fn=<MulBackward0>)}
[25:6239]  loss_total_vae=10791.504  loss_recon=6733.528  loss_kld=725.757  loss_prediction=774.873  loss_true_values=2557.345  loss_total_vae_epoch=11011.714  
tracking changes
Validation stop evaluation
6239 25
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
Saved model at epoch 26
Losses: {'total_vae': tensor(10810.2861, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6699.0942, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(709.0818, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(727.6389, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2674.4719, device='cuda:0', grad_fn=<MulBackward0>)}
[26:6479]  loss_total_vae=10805.291  loss_recon=6773.440  loss_kld=728.310  loss_prediction=749.386  loss_true_values=2554.155  loss_total_vae_epoch=10999.245  
tracking changes
Validation stop evaluation
6479 26
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
Now val counter at: 1
Latent 0.37740686237812043   0.38290357424153215
BCE 0.8691748340924581 0.8847958193884956
Saved model at epoch 27
Losses: {'total_vae': tensor(10937.1055, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6691.6675, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(772.1149, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(815.2287, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2658.0945, device='cuda:0', grad_fn=<MulBackward0>)}
[27:6719]  loss_total_vae=10823.108  loss_recon=6571.619  loss_kld=784.941  loss_prediction=849.753  loss_true_values=2616.796  loss_total_vae_epoch=10980.270  
tracking changes
Validation stop evaluation
6719 27
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
Saved model at epoch 28
Losses: {'total_vae': tensor(11114.5889, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6734.8613, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(776.5408, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(974.1508, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2629.0361, device='cuda:0', grad_fn=<MulBackward0>)}
[28:6959]  loss_total_vae=11062.104  loss_recon=6752.950  loss_kld=749.273  loss_prediction=869.761  loss_true_values=2690.119  loss_total_vae_epoch=10969.027  
tracking changes
Validation stop evaluation
6959 28
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
Now val counter at: 1
Latent 0.3778288245201111   0.3812421527173784
BCE 0.8562291463216146 0.8746959957811568
Saved model at epoch 29
Losses: {'total_vae': tensor(10895.3838, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6678.7178, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(772.8605, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(901.2952, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2542.5107, device='cuda:0', grad_fn=<MulBackward0>)}
[29:7199]  loss_total_vae=10888.576  loss_recon=6681.072  loss_kld=785.981  loss_prediction=847.052  loss_true_values=2574.472  loss_total_vae_epoch=10951.675  
tracking changes
Validation stop evaluation
7199 29
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
Now val counter at: 1
Latent 0.37785121897856394   0.37731157441933955
BCE 0.8499017477035522 0.8670388314459059
Saved model at epoch 30
Losses: {'total_vae': tensor(10954.6514, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6717.6147, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(758.6010, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(919.4230, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2559.0129, device='cuda:0', grad_fn=<MulBackward0>)}
[30:7439]  loss_total_vae=11077.784  loss_recon=6846.927  loss_kld=742.837  loss_prediction=777.177  loss_true_values=2710.843  loss_total_vae_epoch=10941.772  
tracking changes
Validation stop evaluation
7439 30
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
Now val counter at: 1
Latent 0.37735363841056824   0.37613069944911537
BCE 0.8542351484298706 0.8630398677455055
Saved model at epoch 31
Losses: {'total_vae': tensor(10911.6201, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6727.7485, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(728.8502, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(842.0146, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2613.0066, device='cuda:0', grad_fn=<MulBackward0>)}
[31:7679]  loss_total_vae=10955.836  loss_recon=6731.018  loss_kld=743.406  loss_prediction=860.275  loss_true_values=2621.137  loss_total_vae_epoch=10927.721  
tracking changes
Validation stop evaluation
7679 31
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
Saved model at epoch 32
Losses: {'total_vae': tensor(10895.2490, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6734.1699, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(788.1875, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(795.2728, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2577.6194, device='cuda:0', grad_fn=<MulBackward0>)}
[32:7919]  loss_total_vae=11122.846  loss_recon=6863.229  loss_kld=733.861  loss_prediction=900.054  loss_true_values=2625.702  loss_total_vae_epoch=10915.558  
tracking changes
Validation stop evaluation
7919 32
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
Now val counter at: 1
Latent 0.3742467006047567   0.37767789396974777
BCE 0.8483226120471954 0.8534553474850125
Saved model at epoch 33
Losses: {'total_vae': tensor(10632.6650, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6621.7061, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(783.3721, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(678.8873, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2548.6997, device='cuda:0', grad_fn=<MulBackward0>)}
[33:8159]  loss_total_vae=10856.041  loss_recon=6666.682  loss_kld=800.927  loss_prediction=810.323  loss_true_values=2578.110  loss_total_vae_epoch=10901.513  
tracking changes
Validation stop evaluation
8159 33
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
Saved model at epoch 34
Losses: {'total_vae': tensor(10732.3965, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6657.1147, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(776.3654, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(750.2138, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2548.7029, device='cuda:0', grad_fn=<MulBackward0>)}
[34:8399]  loss_total_vae=10908.170  loss_recon=6796.860  loss_kld=748.625  loss_prediction=774.992  loss_true_values=2587.693  loss_total_vae_epoch=10887.552  
tracking changes
Validation stop evaluation
8399 34
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
Now val counter at: 1
Latent 0.37299689451853435   0.37552061975002293
BCE 0.8672752062479655 0.8499829583697848
Saved model at epoch 35
Losses: {'total_vae': tensor(10855.9844, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6670.5146, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(740.1990, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(738.9619, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2706.3081, device='cuda:0', grad_fn=<MulBackward0>)}
[35:8639]  loss_total_vae=10914.570  loss_recon=6802.581  loss_kld=754.343  loss_prediction=784.388  loss_true_values=2573.258  loss_total_vae_epoch=10877.623  
tracking changes
Validation stop evaluation
8639 35
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
Saved model at epoch 36
Losses: {'total_vae': tensor(11140.6357, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6895.8394, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(763.8379, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(900.1532, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2580.8054, device='cuda:0', grad_fn=<MulBackward0>)}
[36:8879]  loss_total_vae=10842.026  loss_recon=6769.759  loss_kld=807.900  loss_prediction=753.820  loss_true_values=2510.548  loss_total_vae_epoch=10866.955  
tracking changes
Validation stop evaluation
8879 36
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
Saved model at epoch 37
Losses: {'total_vae': tensor(10830.8818, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6803.9028, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(778.3937, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(731.5114, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2517.0745, device='cuda:0', grad_fn=<MulBackward0>)}
[37:9119]  loss_total_vae=10745.238  loss_recon=6752.549  loss_kld=769.115  loss_prediction=754.266  loss_true_values=2469.308  loss_total_vae_epoch=10859.520  
tracking changes
Validation stop evaluation
9119 37
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
Now val counter at: 1
Latent 0.37055773437023165   0.37232685519589315
BCE 0.8538874924182892 0.8479606621795229
Saved model at epoch 38
Losses: {'total_vae': tensor(10917.8193, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6692.0942, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(802.9459, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(906.8983, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2515.8809, device='cuda:0', grad_fn=<MulBackward0>)}
[38:9359]  loss_total_vae=10933.348  loss_recon=6802.983  loss_kld=770.167  loss_prediction=802.444  loss_true_values=2557.753  loss_total_vae_epoch=10847.601  
tracking changes
Validation stop evaluation
9359 38
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
Saved model at epoch 39
Losses: {'total_vae': tensor(11002.1133, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6817.3569, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(780.0162, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(794.1960, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2610.5439, device='cuda:0', grad_fn=<MulBackward0>)}
[39:9599]  loss_total_vae=10817.664  loss_recon=6725.166  loss_kld=728.713  loss_prediction=756.775  loss_true_values=2607.010  loss_total_vae_epoch=10836.274  
tracking changes
Validation stop evaluation
9599 39
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
Now val counter at: 1
Latent 0.37967142363389333   0.3709653463628557
BCE 0.8341166317462921 0.8400049580468072
Saved model at epoch 40
Losses: {'total_vae': tensor(10925.9307, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6649.5522, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(705.4775, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(905.0280, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2665.8733, device='cuda:0', grad_fn=<MulBackward0>)}
[40:9839]  loss_total_vae=10845.866  loss_recon=6730.025  loss_kld=794.101  loss_prediction=756.799  loss_true_values=2564.941  loss_total_vae_epoch=10824.694  
tracking changes
Validation stop evaluation
9839 40
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
Saved model at epoch 41
Losses: {'total_vae': tensor(10758.7959, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6649.9287, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(785.7373, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(761.9633, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2561.1670, device='cuda:0', grad_fn=<MulBackward0>)}
[41:10079]  loss_total_vae=10767.290  loss_recon=6703.753  loss_kld=772.541  loss_prediction=738.760  loss_true_values=2552.235  loss_total_vae_epoch=10814.307  
tracking changes
Validation stop evaluation
10079 41
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
Now val counter at: 1
Latent 0.3669960031906764   0.37352145546012455
BCE 0.8325901707013448 0.8436163511541155
Saved model at epoch 42
Losses: {'total_vae': tensor(10861.7734, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6696.0488, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(799.7080, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(887.6648, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2478.3521, device='cuda:0', grad_fn=<MulBackward0>)}
[42:10319]  loss_total_vae=10790.930  loss_recon=6837.098  loss_kld=767.389  loss_prediction=656.627  loss_true_values=2529.815  loss_total_vae_epoch=10804.587  
tracking changes
Validation stop evaluation
10319 42
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
Now val counter at: 1
Latent 0.36962447365125023   0.3731818957461251
BCE 0.8264953752358755 0.835140480597814
Saved model at epoch 43
Losses: {'total_vae': tensor(10855.5459, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6769.5337, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(746.7095, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(784.3355, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2554.9668, device='cuda:0', grad_fn=<MulBackward0>)}
[43:10559]  loss_total_vae=10708.751  loss_recon=6658.734  loss_kld=760.445  loss_prediction=759.330  loss_true_values=2530.242  loss_total_vae_epoch=10794.661  
tracking changes
Validation stop evaluation
10559 43
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
Now val counter at: 1
Latent 0.37033806641896566   0.37206882735093433
BCE 0.8254584272702535 0.8317222277323405
Saved model at epoch 44
Losses: {'total_vae': tensor(10862.2070, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6780.0010, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(751.8756, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(744.6487, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2585.6812, device='cuda:0', grad_fn=<MulBackward0>)}
[44:10799]  loss_total_vae=10915.586  loss_recon=6702.306  loss_kld=781.641  loss_prediction=882.505  loss_true_values=2549.135  loss_total_vae_epoch=10788.429  
tracking changes
Validation stop evaluation
10799 44
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
Now val counter at: 1
Latent 0.37273234824339546   0.3687198440233866
BCE 0.820667439699173 0.829181808895535
Saved model at epoch 45
Losses: {'total_vae': tensor(10818.9756, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6774.6816, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(748.0297, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(754.8800, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2541.3838, device='cuda:0', grad_fn=<MulBackward0>)}
[45:11039]  loss_total_vae=10803.026  loss_recon=6768.619  loss_kld=775.693  loss_prediction=680.200  loss_true_values=2578.515  loss_total_vae_epoch=10775.757  
tracking changes
Validation stop evaluation
11039 45
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
Now val counter at: 1
Latent 0.37132534285386404   0.3689861810869641
BCE 0.8324395020802816 0.8281813244024913
Losses: {'total_vae': tensor(11026.6406, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6832.4463, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(746.0546, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(808.9412, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2639.1987, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
11279 46
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
Now val counter at: 2
Latent 0.37227683862050376   0.3708982961045371
BCE 0.8460012594858806 0.8242070807351006
Losses: {'total_vae': tensor(10662.9941, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6622.4336, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(741.9049, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(772.0037, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2526.6523, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
11519 47
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
Latent 0.3662458101908366   0.37146525250540835
BCE 0.8142147441705068 0.8261884563499028
Saved model at epoch 48
Losses: {'total_vae': tensor(10822.7656, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6786.1631, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(770.6549, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(702.8498, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2563.0972, device='cuda:0', grad_fn=<MulBackward0>)}
[48:11759]  loss_total_vae=10922.792  loss_recon=6815.711  loss_kld=776.823  loss_prediction=833.140  loss_true_values=2497.118  loss_total_vae_epoch=10751.171  
tracking changes
Validation stop evaluation
11759 48
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
Now val counter at: 1
Latent 0.36920416553815205   0.37211150990592107
BCE 0.8336125055948893 0.833036067088445
Saved model at epoch 49
Losses: {'total_vae': tensor(10737.0117, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6758.7686, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(732.6935, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(692.8839, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2552.6660, device='cuda:0', grad_fn=<MulBackward0>)}
[49:11999]  loss_total_vae=10492.199  loss_recon=6565.068  loss_kld=798.355  loss_prediction=690.882  loss_true_values=2437.894  loss_total_vae_epoch=10746.401  
tracking changes
Validation stop evaluation
11999 49
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
Saved model at epoch 50
Losses: {'total_vae': tensor(10778.9473, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6669.9917, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(811.4692, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(809.7332, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2487.7532, device='cuda:0', grad_fn=<MulBackward0>)}
[50:12239]  loss_total_vae=10783.779  loss_recon=6742.630  loss_kld=764.714  loss_prediction=814.295  loss_true_values=2462.141  loss_total_vae_epoch=10734.067  
tracking changes
Validation stop evaluation
12239 50
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
Now val counter at: 1
Latent 0.3647269030412038   0.36924227144983074
BCE 0.818142690261205 0.8312761697504255
Saved model at epoch 51
Losses: {'total_vae': tensor(10932.4609, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6796.7524, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(793.5762, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(743.4258, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2598.7065, device='cuda:0', grad_fn=<MulBackward0>)}
[51:12479]  loss_total_vae=10612.117  loss_recon=6573.604  loss_kld=748.819  loss_prediction=733.928  loss_true_values=2555.766  loss_total_vae_epoch=10718.619  
tracking changes
Validation stop evaluation
12479 51
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
Now val counter at: 1
Latent 0.3660782307386398   0.36638040178351927
BCE 0.8209039608637492 0.824231966998842
Saved model at epoch 52
Losses: {'total_vae': tensor(10514.2969, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6583.4697, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(799.2113, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(651.5413, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2480.0747, device='cuda:0', grad_fn=<MulBackward0>)}
[52:12719]  loss_total_vae=10764.026  loss_recon=6764.152  loss_kld=777.408  loss_prediction=736.433  loss_true_values=2486.032  loss_total_vae_epoch=10714.859  
tracking changes
Validation stop evaluation
12719 52
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
Now val counter at: 1
Latent 0.3703018695116043   0.3658740994003084
BCE 0.8282617588837942 0.8255412823624081
Losses: {'total_vae': tensor(10667.0693, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6685.3467, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(760.7895, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(758.7215, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2462.2119, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
12959 53
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
Now val counter at: 2
Latent 0.3640108396609624   0.3648321211338043
BCE 0.8289403140544891 0.8213051007853615
Saved model at epoch 54
Losses: {'total_vae': tensor(10925.3047, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6768.3301, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(790.4456, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(815.9786, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2550.5510, device='cuda:0', grad_fn=<MulBackward0>)}
[54:13199]  loss_total_vae=10610.385  loss_recon=6575.060  loss_kld=802.500  loss_prediction=736.161  loss_true_values=2496.664  loss_total_vae_epoch=10699.294  
tracking changes
Validation stop evaluation
13199 54
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
Now val counter at: 1
Latent 0.36580322484175365   0.36703566776381596
BCE 0.8318928976853689 0.8224361366695828
Saved model at epoch 55
Losses: {'total_vae': tensor(10609.8838, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6643.2666, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(778.9114, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(696.8525, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2490.8533, device='cuda:0', grad_fn=<MulBackward0>)}
[55:13439]  loss_total_vae=10574.386  loss_recon=6717.916  loss_kld=800.750  loss_prediction=623.457  loss_true_values=2432.264  loss_total_vae_epoch=10692.020  
tracking changes
Validation stop evaluation
13439 55
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
Now val counter at: 1
Latent 0.36739588479200996   0.3667969799704022
BCE 0.8275190969308217 0.8260353446006775
Losses: {'total_vae': tensor(10719.2080, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6783.6641, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(763.7697, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(678.8446, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2492.9297, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
13679 56
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
Latent 0.365312260389328   0.36670531133810674
BCE 0.819621221224467 0.8296983235412174
Saved model at epoch 57
Losses: {'total_vae': tensor(10653.3467, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6704.2104, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(777.1708, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(728.9709, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2442.9944, device='cuda:0', grad_fn=<MulBackward0>)}
[57:13919]  loss_total_vae=10818.723  loss_recon=6649.147  loss_kld=827.639  loss_prediction=855.847  loss_true_values=2486.089  loss_total_vae_epoch=10676.412  
tracking changes
Validation stop evaluation
13919 57
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
Now val counter at: 1
Latent 0.3653734525044759   0.36573664976490866
BCE 0.822950013478597 0.8294507695568932
Saved model at epoch 58
Losses: {'total_vae': tensor(10566.0215, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6690.6538, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(795.1884, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(606.2900, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2473.8889, device='cuda:0', grad_fn=<MulBackward0>)}
[58:14159]  loss_total_vae=10860.734  loss_recon=6806.447  loss_kld=796.437  loss_prediction=756.610  loss_true_values=2501.240  loss_total_vae_epoch=10666.396  
tracking changes
Validation stop evaluation
14159 58
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
Saved model at epoch 59
Losses: {'total_vae': tensor(10481.9844, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6455.3198, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(772.4893, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(720.4653, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2533.7097, device='cuda:0', grad_fn=<MulBackward0>)}
[59:14399]  loss_total_vae=10754.329  loss_recon=6649.979  loss_kld=792.988  loss_prediction=777.697  loss_true_values=2533.665  loss_total_vae_epoch=10660.549  
tracking changes
Validation stop evaluation
14399 59
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
30   31.0  6756.539521  738.518719  0.377354  0.854235  0.806034
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
Now val counter at: 1
Latent 0.3610817770163218   0.3660271992286046
BCE 0.8228910366694132 0.8233634438779619
Saved model at epoch 60
Losses: {'total_vae': tensor(10696.3047, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6687.0591, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(817.5493, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(663.2501, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2528.4460, device='cuda:0', grad_fn=<MulBackward0>)}
[60:14639]  loss_total_vae=10776.129  loss_recon=6652.479  loss_kld=802.578  loss_prediction=799.936  loss_true_values=2521.136  loss_total_vae_epoch=10652.111  
tracking changes
Validation stop evaluation
14639 60
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
31   32.0  6746.194063  754.679628  0.374962  0.847391  0.809163
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765

[61 rows x 6 columns]
Saved model at epoch 61
Losses: {'total_vae': tensor(10473.5908, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6558.7798, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(830.6412, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(678.4461, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2405.7234, device='cuda:0', grad_fn=<MulBackward0>)}
[61:14879]  loss_total_vae=10676.117  loss_recon=6754.335  loss_kld=796.660  loss_prediction=709.608  loss_true_values=2415.514  loss_total_vae_epoch=10647.140  
tracking changes
Validation stop evaluation
14879 61
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
32   33.0  6740.808375  765.781138  0.374247  0.848323  0.811339
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796

[62 rows x 6 columns]
Now val counter at: 1
Latent 0.36378048261006674   0.36330519881513385
BCE 0.8292907913525899 0.8212993767526414
Losses: {'total_vae': tensor(10650.6240, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6672.5898, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(793.5474, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(780.0517, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2404.4353, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
15119 62
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
33   34.0  6734.363708  780.084859  0.371781  0.839763  0.811275
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940

[63 rows x 6 columns]
Latent 0.36041694283485415   0.3613413022624122
BCE 0.8237290481726328 0.8171902338663738
Saved model at epoch 63
Losses: {'total_vae': tensor(10762.8438, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6697.7959, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(826.3453, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(757.7632, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2480.9397, device='cuda:0', grad_fn=<MulBackward0>)}
[63:15359]  loss_total_vae=10739.432  loss_recon=6700.148  loss_kld=789.542  loss_prediction=770.838  loss_true_values=2478.904  loss_total_vae_epoch=10629.704  
tracking changes
Validation stop evaluation
15359 63
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
34   35.0  6738.269604  770.627299  0.372997  0.867275  0.806892
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146

[64 rows x 6 columns]
Now val counter at: 1
Latent 0.363000691930453   0.36144800749089984
BCE 0.8333504974842072 0.820934804280599
Losses: {'total_vae': tensor(10467.4727, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6585.4238, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(785.5576, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(637.3352, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2459.1560, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
15599 64
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
35   36.0  6736.797333  768.263645  0.372202  0.836844  0.813877
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380

[65 rows x 6 columns]
Latent 0.36096588571866356   0.36122639609707735
BCE 0.8259509106477102 0.8212141414483387
Saved model at epoch 65
Losses: {'total_vae': tensor(10577.0908, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6705.1016, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(835.6745, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(647.6036, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2388.7117, device='cuda:0', grad_fn=<MulBackward0>)}
[65:15839]  loss_total_vae=10582.441  loss_recon=6744.982  loss_kld=768.070  loss_prediction=625.859  loss_true_values=2443.530  loss_total_vae_epoch=10608.924  
tracking changes
Validation stop evaluation
15839 65
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
36   37.0  6734.452979  780.547929  0.370136  0.829284  0.814726
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621

[66 rows x 6 columns]
Now val counter at: 1
Latent 0.3623200416564941   0.36239937245845794
BCE 0.8374356846014659 0.8287901123364767
Saved model at epoch 66
Losses: {'total_vae': tensor(10809.5840, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6855.1587, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(802.9224, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(702.5728, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2448.9292, device='cuda:0', grad_fn=<MulBackward0>)}
[66:16079]  loss_total_vae=10545.534  loss_recon=6702.609  loss_kld=824.311  loss_prediction=574.418  loss_true_values=2444.195  loss_total_vae_epoch=10604.759  
tracking changes
Validation stop evaluation
16079 66
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
37   38.0  6729.538875  773.370884  0.370558  0.853887  0.808972
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506

[67 rows x 6 columns]
Now val counter at: 1
Latent 0.3621455530325572   0.3614611734946569
BCE 0.8383792877197266 0.8276768187681834
Losses: {'total_vae': tensor(10456.5781, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6691.5488, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(812.4252, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(586.1534, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2366.4509, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
16319 67
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
38   39.0  6723.214813  776.456801  0.370335  0.842845  0.815660
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229

[68 rows x 6 columns]
Now val counter at: 2
Latent 0.36548575162887575   0.36209553976853687
BCE 0.8272229075431824 0.8322456975777944
Saved model at epoch 68
Losses: {'total_vae': tensor(10498.2773, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6761.4707, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(757.0084, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(472.2597, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2507.5388, device='cuda:0', grad_fn=<MulBackward0>)}
[68:16559]  loss_total_vae=10578.271  loss_recon=6609.338  loss_kld=781.579  loss_prediction=763.140  loss_true_values=2424.214  loss_total_vae_epoch=10588.450  
tracking changes
Validation stop evaluation
16559 68
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
39   40.0  6738.080062  704.052043  0.379671  0.834117  0.815868
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524

[69 rows x 6 columns]
Now val counter at: 1
Latent 0.3627640068531036   0.3618104934692383
BCE 0.8306754469871521 0.8339219609896342
Saved model at epoch 69
Losses: {'total_vae': tensor(10508.3740, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6650.7847, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(796.2341, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(606.3843, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2454.9712, device='cuda:0', grad_fn=<MulBackward0>)}
[69:16799]  loss_total_vae=10598.948  loss_recon=6719.344  loss_kld=790.071  loss_prediction=647.571  loss_true_values=2441.962  loss_total_vae_epoch=10580.267  
tracking changes
Validation stop evaluation
16799 69
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
40   41.0  6719.356417  776.684338  0.369539  0.828460  0.817493
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886

[70 rows x 6 columns]
Now val counter at: 1
Latent 0.3641253064076106   0.36331711543930895
BCE 0.8343649427096049 0.8343459599547915
Losses: {'total_vae': tensor(10511.1494, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6647.5918, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(785.9756, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(642.1011, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2435.4812, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
17039 70
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
41   42.0  6721.774646  795.926849  0.366996  0.832590  0.816118
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089

[71 rows x 6 columns]
Latent 0.35863315959771475   0.3634651038381788
BCE 0.8315978229045868 0.832092547416687
Saved model at epoch 71
Losses: {'total_vae': tensor(10589.9668, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6789.0635, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(856.7650, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(518.3097, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2425.8286, device='cuda:0', grad_fn=<MulBackward0>)}
[71:17279]  loss_total_vae=10788.762  loss_recon=6850.709  loss_kld=761.478  loss_prediction=721.438  loss_true_values=2455.136  loss_total_vae_epoch=10569.503  
tracking changes
Validation stop evaluation
17279 71
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
42   43.0  6721.507146  762.136310  0.369624  0.826495  0.817790
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810

[72 rows x 6 columns]
Now val counter at: 1
Latent 0.36657253205776213   0.3641250216298633
BCE 0.845040621360143 0.8307544324133133
Losses: {'total_vae': tensor(10461.1113, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6567.6387, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(773.6325, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(688.1611, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2431.6794, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
17519 72
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
43   44.0  6720.185125  763.121590  0.370338  0.825458  0.817976
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622

[73 rows x 6 columns]
Latent 0.3631199618180593   0.361840824286143
BCE 0.8401648024717967 0.8322127375337812
Losses: {'total_vae': tensor(10820.7373, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6779.6865, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(743.9723, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(724.8003, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2572.2781, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
17759 73
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
44   45.0  6720.367312  740.275378  0.372732  0.820667  0.817994
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997

[74 rows x 6 columns]
Now val counter at: 2
Latent 0.3606722891330719   0.3631103326876958
BCE 0.845684814453125 0.837001128991445
Saved model at epoch 74
Losses: {'total_vae': tensor(10428.2080, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6697.4380, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(820.7195, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(538.2057, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2371.8445, device='cuda:0', grad_fn=<MulBackward0>)}
[74:17999]  loss_total_vae=10498.151  loss_recon=6689.084  loss_kld=790.002  loss_prediction=623.679  loss_true_values=2395.387  loss_total_vae_epoch=10545.701  
tracking changes
Validation stop evaluation
17999 74
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
45   46.0  6713.462937  754.569621  0.371325  0.832440  0.819731
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828

[75 rows x 6 columns]
Now val counter at: 1
Latent 0.3638765215873718   0.36277521782451205
BCE 0.8299012601375579 0.8389344155788422
Saved model at epoch 75
Losses: {'total_vae': tensor(10595.4102, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6703.7490, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(776.6448, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(662.2282, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2452.7881, device='cuda:0', grad_fn=<MulBackward0>)}
[75:18239]  loss_total_vae=10535.773  loss_recon=6540.663  loss_kld=777.044  loss_prediction=701.247  loss_true_values=2516.820  loss_total_vae_epoch=10535.405  
tracking changes
Validation stop evaluation
18239 75
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
46   47.0  6720.286750  739.967714  0.372277  0.846001  0.813744
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638

[76 rows x 6 columns]
Now val counter at: 1
Latent 0.36067137519518533   0.3634549276696311
BCE 0.8458369394143422 0.843630079428355
Saved model at epoch 76
Losses: {'total_vae': tensor(10341.0254, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6550.8618, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(820.1229, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(550.1641, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2419.8765, device='cuda:0', grad_fn=<MulBackward0>)}
[76:18479]  loss_total_vae=10430.046  loss_recon=6630.399  loss_kld=766.562  loss_prediction=550.141  loss_true_values=2482.943  loss_total_vae_epoch=10529.648  
tracking changes
Validation stop evaluation
18479 76
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
47   48.0  6707.291854  780.158588  0.366246  0.814215  0.820290
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906

[77 rows x 6 columns]
Now val counter at: 1
Latent 0.35862334767977394   0.3625562575128343
BCE 0.8462341666221619 0.8385836256874932
Saved model at epoch 77
Losses: {'total_vae': tensor(10413.8936, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6547.9160, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(819.1659, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(600.9562, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2445.8557, device='cuda:0', grad_fn=<MulBackward0>)}
[77:18719]  loss_total_vae=10572.084  loss_recon=6746.478  loss_kld=807.189  loss_prediction=580.764  loss_true_values=2437.654  loss_total_vae_epoch=10524.239  
tracking changes
Validation stop evaluation
18719 77
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
48   49.0  6717.521562  755.202533  0.369204  0.833613  0.815516
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516

[78 rows x 6 columns]
Now val counter at: 1
Latent 0.3618541290362676   0.3617400619718764
BCE 0.8431658287843068 0.840474338001675
Losses: {'total_vae': tensor(10360.3809, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6529.3599, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(807.8186, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(616.3036, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2406.8984, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
18959 78
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
49   50.0  6700.785917  811.487563  0.363691  0.824869  0.819839
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113

[79 rows x 6 columns]
Now val counter at: 2
Latent 0.3695475061734517   0.3610570814874437
BCE 0.8502904872099558 0.8406574553913541
Losses: {'total_vae': tensor(10460.7852, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6650.6270, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(738.3386, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(586.6453, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2485.1746, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
19199 79
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
50   51.0  6709.244208  780.801955  0.364727  0.818143  0.820462
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077

[80 rows x 6 columns]
Now val counter at: 3
Latent 0.36166917979717256   0.36038295063707565
BCE 0.8518043637275696 0.8450789782736036
Losses: {'total_vae': tensor(10408.5234, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6583.2456, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(803.4620, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(614.7983, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2407.0176, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
19439 80
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
51   52.0  6702.885042  779.226308  0.366078  0.820904  0.821975
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533

[81 rows x 6 columns]
Now val counter at: 4
Latent 0.36244185666243234   0.3633416609631644
BCE 0.849137258529663 0.8465634942054748
Saved model at epoch 81
Losses: {'total_vae': tensor(10608.8311, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6795.3105, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(773.6865, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(601.6339, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2438.2000, device='cuda:0', grad_fn=<MulBackward0>)}
[81:19679]  loss_total_vae=10364.321  loss_recon=6687.998  loss_kld=817.839  loss_prediction=534.608  loss_true_values=2323.876  loss_total_vae_epoch=10492.046  
tracking changes
Validation stop evaluation
19679 81
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
52   53.0  6710.429958  747.862136  0.370302  0.828262  0.823044
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699

[82 rows x 6 columns]
Now val counter at: 1
Latent 0.3627072811126709   0.36435693833563065
BCE 0.8488778531551361 0.8484202265739441
Saved model at epoch 82
Losses: {'total_vae': tensor(10388.0215, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6633.0742, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(786.9565, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(549.2440, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2418.7468, device='cuda:0', grad_fn=<MulBackward0>)}
[82:19919]  loss_total_vae=10658.504  loss_recon=6855.992  loss_kld=778.354  loss_prediction=593.702  loss_true_values=2430.457  loss_total_vae_epoch=10489.031  
tracking changes
Validation stop evaluation
19919 82
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
53   54.0  6702.736583  800.942234  0.364011  0.828940  0.823064
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020

[83 rows x 6 columns]
Now val counter at: 1
Latent 0.3601865271727244   0.36455284754435224
BCE 0.8616357723871867 0.8504107031557296
Saved model at epoch 83
Losses: {'total_vae': tensor(10527.4844, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6748.8984, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(807.0822, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(560.9280, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2410.5759, device='cuda:0', grad_fn=<MulBackward0>)}
[83:20159]  loss_total_vae=10319.830  loss_recon=6617.508  loss_kld=814.438  loss_prediction=483.182  loss_true_values=2404.701  loss_total_vae_epoch=10474.609  
tracking changes
Validation stop evaluation
20159 83
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
54   55.0  6705.240625  784.084491  0.365803  0.831893  0.820557
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900

[84 rows x 6 columns]
Now val counter at: 1
Latent 0.360797510544459   0.3622727725240919
BCE 0.8561723510424296 0.8499398251374563
Saved model at epoch 84
Losses: {'total_vae': tensor(10383.5771, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6718.4136, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(823.3484, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(539.2062, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2302.6094, device='cuda:0', grad_fn=<MulBackward0>)}
[84:20399]  loss_total_vae=10360.068  loss_recon=6602.710  loss_kld=858.762  loss_prediction=518.920  loss_true_values=2379.677  loss_total_vae_epoch=10469.899  
tracking changes
Validation stop evaluation
20399 84
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
55   56.0  6710.264687  756.725863  0.367396  0.827519  0.820082
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123

[85 rows x 6 columns]
Now val counter at: 1
Latent 0.3606358140707016   0.3617785549826092
BCE 0.8600727876027425 0.8532169613573286
Saved model at epoch 85
Losses: {'total_vae': tensor(10340.2012, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6666.5249, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(806.3135, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(486.9196, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2380.4438, device='cuda:0', grad_fn=<MulBackward0>)}
[85:20639]  loss_total_vae=10436.236  loss_recon=6680.809  loss_kld=798.156  loss_prediction=524.855  loss_true_values=2432.417  loss_total_vae_epoch=10464.386  
tracking changes
Validation stop evaluation
20639 85
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
56   57.0  6704.673104  774.223301  0.365312  0.819621  0.822645
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427

[86 rows x 6 columns]
Now val counter at: 1
Latent 0.36039925515651705   0.36123043960995144
BCE 0.8650551974773407 0.8555619921949175
Saved model at epoch 86
Losses: {'total_vae': tensor(10588.5029, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6862.7700, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(812.2053, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(462.5685, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2450.9595, device='cuda:0', grad_fn=<MulBackward0>)}
[86:20879]  loss_total_vae=10652.339  loss_recon=6844.134  loss_kld=776.771  loss_prediction=552.749  loss_true_values=2478.685  loss_total_vae_epoch=10458.916  
tracking changes
Validation stop evaluation
20879 86
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
57   58.0  6697.235313  785.930479  0.365373  0.822950  0.824173
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015

[87 rows x 6 columns]
Now val counter at: 1
Latent 0.3634426802396774   0.36053995059596167
BCE 0.8681743681430817 0.8592936370107863
Losses: {'total_vae': tensor(10647.2969, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6795.1694, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(774.8847, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(607.3898, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2469.8525, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
21119 87
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
58   59.0  6694.499104  790.057762  0.363460  0.818057  0.823479
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948

[88 rows x 6 columns]
Now val counter at: 2
Latent 0.3570527484019597   0.3606108599238926
BCE 0.8807468036810557 0.8604334453741709
Saved model at epoch 88
Losses: {'total_vae': tensor(10520.0137, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6658.7642, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(888.8677, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(604.3918, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2367.9897, device='cuda:0', grad_fn=<MulBackward0>)}
[88:21359]  loss_total_vae=10456.535  loss_recon=6655.672  loss_kld=805.617  loss_prediction=588.756  loss_true_values=2406.490  loss_total_vae_epoch=10443.018  
tracking changes
Validation stop evaluation
21359 88
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
59   60.0  6694.691083  816.068392  0.361082  0.822891  0.824159
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827

[89 rows x 6 columns]
Now val counter at: 1
Latent 0.3607668419679006   0.3614925831556321
BCE 0.8966510514418284 0.8644341177410549
Saved model at epoch 89
Losses: {'total_vae': tensor(10434.0850, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6738.5996, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(806.3849, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(459.7405, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2429.3601, device='cuda:0', grad_fn=<MulBackward0>)}
[89:21599]  loss_total_vae=10617.094  loss_recon=6821.715  loss_kld=810.524  loss_prediction=578.389  loss_true_values=2406.465  loss_total_vae_epoch=10438.949  
tracking changes
Validation stop evaluation
21599 89
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
60   61.0  6690.368229  836.312193  0.359482  0.810623  0.828765
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403

[90 rows x 6 columns]
Saved model at epoch 90
Losses: {'total_vae': tensor(10226.2373, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6609.0029, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(836.3549, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(394.6639, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2386.2156, device='cuda:0', grad_fn=<MulBackward0>)}
[90:21839]  loss_total_vae=10327.622  loss_recon=6633.257  loss_kld=836.996  loss_prediction=478.783  loss_true_values=2378.587  loss_total_vae_epoch=10426.710  
tracking changes
Validation stop evaluation
21839 90
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
61   62.0  6699.925562  787.878776  0.363780  0.829291  0.820796
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620

[91 rows x 6 columns]
Now val counter at: 1
Latent 0.3564457297325134   0.3604207568698459
BCE 0.8837484697500865 0.881857407755322
Saved model at epoch 91
Losses: {'total_vae': tensor(10372.7256, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6601.0361, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(834.5307, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(531.7803, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2405.3787, device='cuda:0', grad_fn=<MulBackward0>)}
[91:22079]  loss_total_vae=10535.623  loss_recon=6704.878  loss_kld=840.694  loss_prediction=599.715  loss_true_values=2390.336  loss_total_vae_epoch=10420.279  
tracking changes
Validation stop evaluation
22079 91
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
62   63.0  6687.198687  821.665849  0.360417  0.823729  0.827940
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91   92.0  6670.573063  866.878282  0.355005  0.889425  0.832915

[92 rows x 6 columns]
Now val counter at: 1
Latent 0.3550049146016439   0.35903325378894807
BCE 0.8894246538480123 0.882823098368115
Saved model at epoch 92
Losses: {'total_vae': tensor(10446.1230, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6592.3872, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(861.8163, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(610.8440, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2381.0750, device='cuda:0', grad_fn=<MulBackward0>)}
[92:22319]  loss_total_vae=10352.715  loss_recon=6633.325  loss_kld=774.199  loss_prediction=505.371  loss_true_values=2439.820  loss_total_vae_epoch=10410.630  
tracking changes
Validation stop evaluation
22319 92
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
63   64.0  6699.713500  795.186281  0.363001  0.833350  0.824146
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91   92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92   93.0  6680.450458  814.532835  0.359180  0.905417  0.830113

[93 rows x 6 columns]
Now val counter at: 1
Latent 0.35917962292830147   0.35883091423246594
BCE 0.9054172197977702 0.8838236537244585
Losses: {'total_vae': tensor(10609.1836, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6743.0298, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(820.8953, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(628.7088, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2416.5498, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
22559 93
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
64   65.0  6694.074958  812.239583  0.360966  0.825951  0.825380
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91   92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92   93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93   94.0  6679.650562  814.510018  0.360629  0.902292  0.833726

[94 rows x 6 columns]
Now val counter at: 2
Latent 0.36062853833039604   0.3569102717770471
BCE 0.9022920926411947 0.8814148545265197
Losses: {'total_vae': tensor(10510.4395, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6701.5161, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(825.8084, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(546.5670, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2436.5476, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
22799 94
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
65   66.0  6692.180479  797.415474  0.362320  0.837436  0.821621
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91   92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92   93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93   94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94   95.0  6680.684542  828.337510  0.360112  0.922665  0.833049

[95 rows x 6 columns]
Now val counter at: 3
Latent 0.3601117173830668   0.35687675575415295
BCE 0.9226654390494029 0.8928634477986229
Losses: {'total_vae': tensor(10416.4951, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6725.6978, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(818.3766, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(466.1748, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2406.2463, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
23039 95
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
66   67.0  6689.056250  802.881630  0.362146  0.838379  0.828506
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91   92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92   93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93   94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94   95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95   96.0  6679.083438  825.909776  0.358907  0.895086  0.836533

[96 rows x 6 columns]
Latent 0.3589068382978439   0.35827102528678045
BCE 0.8950864454110463 0.8990446554289924
Saved model at epoch 96
Losses: {'total_vae': tensor(10534.6768, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6623.8145, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(841.3168, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(649.8782, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2419.6672, device='cuda:0', grad_fn=<MulBackward0>)}
[96:23279]  loss_total_vae=10491.944  loss_recon=6774.206  loss_kld=839.997  loss_prediction=504.947  loss_true_values=2372.794  loss_total_vae_epoch=10387.284  
tracking changes
Validation stop evaluation
23279 96
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
67   68.0  6696.434688  768.195030  0.365486  0.827223  0.828229
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91   92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92   93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93   94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94   95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95   96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96   97.0  6679.452646  817.695233  0.359709  0.889434  0.833368

[97 rows x 6 columns]
Now val counter at: 1
Latent 0.35970933437347413   0.3599732928805881
BCE 0.8894335567951203 0.9101249171627893
Saved model at epoch 97
Losses: {'total_vae': tensor(10285.7578, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6658.9810, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(833.6987, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(437.9594, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2355.1189, device='cuda:0', grad_fn=<MulBackward0>)}
[97:23519]  loss_total_vae=10228.526  loss_recon=6548.217  loss_kld=814.174  loss_prediction=440.078  loss_true_values=2426.058  loss_total_vae_epoch=10377.392  
tracking changes
Validation stop evaluation
23519 97
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
68   69.0  6697.259187  798.004716  0.362764  0.830675  0.828524
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91   92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92   93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93   94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94   95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95   96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96   97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97   98.0  6690.765771  772.269674  0.365738  0.919301  0.832793

[98 rows x 6 columns]
Now val counter at: 1
Latent 0.36573781768480934   0.3598823646704356
BCE 0.9193014701207479 0.906681325700548
Losses: {'total_vae': tensor(10502.6230, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6862.3584, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(776.6748, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(447.7447, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2415.8450, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
23759 98
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
69   70.0  6694.376438  785.816345  0.364125  0.834365  0.829886
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91   92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92   93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93   94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94   95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95   96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96   97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97   98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98   99.0  6680.689875  804.526436  0.362104  0.920695  0.831185

[99 rows x 6 columns]
Now val counter at: 2
Latent 0.36210381984710693   0.35957596335146164
BCE 0.9206950505574544 0.9023951470851898
Losses: {'total_vae': tensor(10270.6621, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6620.7124, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(805.1838, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(447.0004, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2397.7649, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
23999 99
    epoch          rec         kld    latent       bce       acc
0     1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1     2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2     3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3     4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4     5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5     6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6     7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7     8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8     9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9    10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10   11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11   12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12   13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13   14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14   15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15   16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16   17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17   18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18   19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19   20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20   21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21   22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22   23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23   24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24   25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25   26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26   27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27   28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28   29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29   30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..    ...          ...         ...       ...       ...       ...
70   71.0  6688.464458  827.761053  0.358633  0.831598  0.829089
71   72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72   73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73   74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74   75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75   76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76   77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77   78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78   79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79   80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80   81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81   82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82   83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83   84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84   85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85   86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86   87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87   88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88   89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89   90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90   91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91   92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92   93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93   94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94   95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95   96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96   97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97   98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98   99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99  100.0  6678.369646  832.635065  0.358057  0.921516  0.834441

[100 rows x 6 columns]
Now val counter at: 3
Latent 0.3580567866563797   0.36145133011870917
BCE 0.9215164840221405 0.9012738241089715
Saved model at epoch 100
Losses: {'total_vae': tensor(10250.2246, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6618.0215, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(826.8903, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(400.2104, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2405.1023, device='cuda:0', grad_fn=<MulBackward0>)}
[100:24239]  loss_total_vae=10453.441  loss_recon=6660.956  loss_kld=842.022  loss_prediction=550.351  loss_true_values=2400.112  loss_total_vae_epoch=10351.637  
tracking changes
Validation stop evaluation
24239 100
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
71    72.0  6701.779792  763.685984  0.366573  0.845041  0.826810
72    73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73    74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74    75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75    76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76    77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77    78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78    79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79    80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282

[101 rows x 6 columns]
Now val counter at: 1
Latent 0.3593452880779902   0.36251699063513015
BCE 0.9127397338549296 0.9098100258244409
Saved model at epoch 101
Losses: {'total_vae': tensor(10239.0488, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6572.9312, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(827.6566, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(457.3339, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2381.1265, device='cuda:0', grad_fn=<MulBackward0>)}
[101:24479]  loss_total_vae=10553.144  loss_recon=6815.074  loss_kld=872.960  loss_prediction=473.907  loss_true_values=2391.203  loss_total_vae_epoch=10346.341  
tracking changes
Validation stop evaluation
24479 101
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
72    73.0  6692.837854  789.763100  0.363120  0.840165  0.827622
73    74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74    75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75    76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76    77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77    78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78    79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79    80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879

[102 rows x 6 columns]
Now val counter at: 1
Latent 0.3556749075651169   0.3619661413960987
BCE 0.9155352691809336 0.9205043349001142
Saved model at epoch 102
Losses: {'total_vae': tensor(10370.6719, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6803.3740, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(845.1927, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(353.4221, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2368.6831, device='cuda:0', grad_fn=<MulBackward0>)}
[102:24719]  loss_total_vae=10423.196  loss_recon=6631.996  loss_kld=842.774  loss_prediction=532.308  loss_true_values=2416.119  loss_total_vae_epoch=10346.502  
tracking changes
Validation stop evaluation
24719 102
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
73    74.0  6683.497667  804.790377  0.360672  0.845685  0.826997
74    75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75    76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76    77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77    78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78    79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79    80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686

[103 rows x 6 columns]
Now val counter at: 1
Latent 0.35925776064395903   0.3598352981938256
BCE 0.9256499429543813 0.9183170894781748
Saved model at epoch 103
Losses: {'total_vae': tensor(10218.0283, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6612.2212, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(845.0971, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(414.6159, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2346.0942, device='cuda:0', grad_fn=<MulBackward0>)}
[103:24959]  loss_total_vae=10244.227  loss_recon=6609.239  loss_kld=834.836  loss_prediction=438.140  loss_true_values=2362.012  loss_total_vae_epoch=10333.438  
tracking changes
Validation stop evaluation
24959 103
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
74    75.0  6690.071437  776.267619  0.363877  0.829901  0.827828
75    76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76    77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77    78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78    79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79    80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680

[104 rows x 6 columns]
Now val counter at: 1
Latent 0.3603580017884572   0.35769232743316226
BCE 0.9297662734985351 0.916597162352668
Losses: {'total_vae': tensor(10214.4805, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6631.9585, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(844.8689, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(406.0229, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2331.6306, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
25199 104
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
75    76.0  6688.469771  811.939299  0.360671  0.845837  0.830638
76    77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77    78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78    79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79    80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849

[105 rows x 6 columns]
Now val counter at: 2
Latent 0.3592321713765462   0.3580926520956887
BCE 0.9421095331509908 0.9179749819967481
Losses: {'total_vae': tensor(10457.2568, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6751.9062, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(822.7321, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(485.6252, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2396.9939, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
25439 105
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
76    77.0  6680.352833  833.534292  0.358623  0.846234  0.828906
77    78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78    79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79    80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901

[106 rows x 6 columns]
Latent 0.35845647752285004   0.3584302233325111
BCE 0.9396927813688914 0.9236504952112834
Losses: {'total_vae': tensor(10330.4688, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6791.4590, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(828.7061, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(311.4572, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2398.8469, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
25679 106
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
77    78.0  6692.826937  785.868083  0.361854  0.843166  0.830516
78    79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79    80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585

[107 rows x 6 columns]
Now val counter at: 3
Latent 0.3636524120966593   0.35961597793632083
BCE 0.9461245556672414 0.9325085832013024
Losses: {'total_vae': tensor(10215.2012, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6612.4468, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(800.5134, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(397.5684, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2404.6726, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
25919 107
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
78    79.0  6695.590792  733.230471  0.369548  0.850290  0.830113
79    80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318

[108 rows x 6 columns]
Now val counter at: 4
Latent 0.35617700616518655   0.3593488835626178
BCE 0.9624435901641846 0.9371895293394724
Saved model at epoch 108
Losses: {'total_vae': tensor(10226.8057, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6648.5859, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(839.9405, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(364.1369, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2374.1428, device='cuda:0', grad_fn=<MulBackward0>)}
[108:26159]  loss_total_vae=10274.947  loss_recon=6675.776  loss_kld=819.316  loss_prediction=360.883  loss_true_values=2418.972  loss_total_vae_epoch=10298.108  
tracking changes
Validation stop evaluation
26159 108
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
79    80.0  6683.746729  792.877405  0.361669  0.851804  0.831077
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530

[109 rows x 6 columns]
Now val counter at: 1
Latent 0.3575099935134252   0.36044702033201853
BCE 0.9591634889443715 0.9426422900623744
Saved model at epoch 109
Losses: {'total_vae': tensor(10190.3076, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6616.2300, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(845.0322, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(416.9144, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2312.1309, device='cuda:0', grad_fn=<MulBackward0>)}
[109:26399]  loss_total_vae=10383.050  loss_recon=6675.405  loss_kld=846.394  loss_prediction=484.928  loss_true_values=2376.323  loss_total_vae_epoch=10289.489  
tracking changes
Validation stop evaluation
26399 109
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
80    81.0  6688.132396  790.439541  0.362442  0.849137  0.830533
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644

[110 rows x 6 columns]
Now val counter at: 1
Latent 0.35757690767447153   0.359428631928232
BCE 0.9645677328109741 0.9494203090667724
Saved model at epoch 110
Losses: {'total_vae': tensor(10287.6084, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6754.6665, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(847.2477, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(334.0016, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2351.6931, device='cuda:0', grad_fn=<MulBackward0>)}
[110:26639]  loss_total_vae=10349.553  loss_recon=6669.661  loss_kld=832.827  loss_prediction=436.392  loss_true_values=2410.673  loss_total_vae_epoch=10283.979  
tracking changes
Validation stop evaluation
26639 110
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
81    82.0  6686.039938  787.311847  0.362707  0.848878  0.832699
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578

[111 rows x 6 columns]
Now val counter at: 1
Latent 0.36014381845792137   0.3591131372584237
BCE 0.9757240533828735 0.9559105449252657
Losses: {'total_vae': tensor(10010.7393, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6510.8091, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(864.8113, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(339.2453, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2295.8735, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
26879 111
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
82    83.0  6696.293792  803.178691  0.360187  0.861636  0.831020
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153

[112 rows x 6 columns]
Now val counter at: 2
Latent 0.3630331367254257   0.3570879691176944
BCE 0.9963093797365824 0.9620582706398434
Losses: {'total_vae': tensor(10399.7080, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6744.0884, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(788.6810, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(449.4286, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2417.5095, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
27119 112
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
83    84.0  6691.065875  805.017657  0.360798  0.856172  0.834900
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985

[113 rows x 6 columns]
Latent 0.3585766126712163   0.3584102398819394
BCE 0.9867322425047557 0.9664850917127398
Losses: {'total_vae': tensor(10181.9648, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6687.1250, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(852.7257, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(379.7953, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2262.3186, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
27359 113
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
84    85.0  6684.703437  804.171663  0.360636  0.860073  0.831123
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674

[114 rows x 6 columns]
Now val counter at: 3
Latent 0.3591286857922872   0.3602512876192729
BCE 0.9812748889128368 0.9788670553101433
Saved model at epoch 114
Losses: {'total_vae': tensor(10316.1836, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6744.4536, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(817.9471, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(388.1802, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2365.6021, device='cuda:0', grad_fn=<MulBackward0>)}
[114:27599]  loss_total_vae=10280.820  loss_recon=6664.253  loss_kld=824.379  loss_prediction=403.076  loss_true_values=2389.111  loss_total_vae_epoch=10251.959  
tracking changes
Validation stop evaluation
27599 114
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
85    86.0  6687.321563  807.224512  0.360399  0.865055  0.832427
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853

[115 rows x 6 columns]
Now val counter at: 1
Latent 0.36052159070968626   0.3605845226181878
BCE 0.9841193636258443 0.9862552252080706
Saved model at epoch 115
Losses: {'total_vae': tensor(10349.4082, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6857.3301, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(846.3160, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(330.2663, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2315.4966, device='cuda:0', grad_fn=<MulBackward0>)}
[115:27839]  loss_total_vae=10261.078  loss_recon=6545.968  loss_kld=835.900  loss_prediction=446.674  loss_true_values=2432.536  loss_total_vae_epoch=10249.124  
tracking changes
Validation stop evaluation
27839 115
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
86    87.0  6688.984563  780.841223  0.363443  0.868174  0.831015
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746

[116 rows x 6 columns]
Now val counter at: 1
Latent 0.3596080323060354   0.36024614506297636
BCE 0.9985583444436391 0.9881055037180583
Saved model at epoch 116
Losses: {'total_vae': tensor(10284.6553, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6705.4512, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(831.8640, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(381.1532, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2366.1870, device='cuda:0', grad_fn=<MulBackward0>)}
[116:28079]  loss_total_vae=10420.529  loss_recon=6816.355  loss_kld=807.993  loss_prediction=379.122  loss_true_values=2417.060  loss_total_vae_epoch=10245.737  
tracking changes
Validation stop evaluation
28079 116
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
87    88.0  6682.453812  856.356203  0.357053  0.880747  0.830948
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258

[117 rows x 6 columns]
Saved model at epoch 117
Losses: {'total_vae': tensor(10361.8818, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6815.8892, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(855.0526, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(287.7550, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2403.1851, device='cuda:0', grad_fn=<MulBackward0>)}
[117:28319]  loss_total_vae=10149.047  loss_recon=6640.066  loss_kld=794.790  loss_prediction=388.744  loss_true_values=2325.447  loss_total_vae_epoch=10233.783  
tracking changes
Validation stop evaluation
28319 117
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
88    89.0  6685.809792  812.392971  0.360767  0.896651  0.831827
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033

[118 rows x 6 columns]
Now val counter at: 1
Latent 0.3609169085820516   0.3597527696026696
BCE 1.0070959826310475 0.9879841989941066
Losses: {'total_vae': tensor(10245.8291, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6685.8203, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(803.9218, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(388.3228, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2367.7644, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
28559 118
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
89    90.0  6677.967958  818.835563  0.359280  0.871071  0.832403
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082

[119 rows x 6 columns]
Now val counter at: 2
Latent 0.36450414955615995   0.359815588593483
BCE 1.0393675247828165 0.9914548973242442
Losses: {'total_vae': tensor(10214.2607, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6648.3682, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(799.1498, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(432.3567, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2334.3860, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
28799 119
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
90    91.0  6678.843771  845.781618  0.356446  0.883748  0.832620
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163

[120 rows x 6 columns]
Latent 0.36024288535118104   0.35994736121760473
BCE 1.0316765089829762 0.9991137703259785
Losses: {'total_vae': tensor(10255.0674, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6680.7388, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(858.5464, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(341.6317, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2374.1506, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
29039 120
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
91    92.0  6670.573063  866.878282  0.355005  0.889425  0.832915
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668

[121 rows x 6 columns]
Now val counter at: 3
Latent 0.3534480581680934   0.3615794003009796
BCE 1.0323208113511404 1.0127168304390377
Saved model at epoch 121
Losses: {'total_vae': tensor(10126.9619, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6593.8506, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(883.4235, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(281.9267, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2367.7610, device='cuda:0', grad_fn=<MulBackward0>)}
[121:29279]  loss_total_vae=10442.473  loss_recon=6776.205  loss_kld=878.953  loss_prediction=430.106  loss_true_values=2357.209  loss_total_vae_epoch=10204.557  
tracking changes
Validation stop evaluation
29279 121
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
92    93.0  6680.450458  814.532835  0.359180  0.905417  0.830113
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787

[122 rows x 6 columns]
Now val counter at: 1
Latent 0.3607839624087016   0.36188798116313087
BCE 1.0363762060801187 1.02604667213228
Saved model at epoch 122
Losses: {'total_vae': tensor(10158.6230, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6633.4565, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(836.2189, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(352.5048, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2336.4431, device='cuda:0', grad_fn=<MulBackward0>)}
[122:29519]  loss_total_vae=10219.218  loss_recon=6617.291  loss_kld=838.145  loss_prediction=363.896  loss_true_values=2399.887  loss_total_vae_epoch=10199.201  
tracking changes
Validation stop evaluation
29519 122
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
93    94.0  6679.650562  814.510018  0.360629  0.902292  0.833726
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863

[123 rows x 6 columns]
Saved model at epoch 123
Losses: {'total_vae': tensor(10215.8721, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6669.9277, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(860.7036, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(408.0171, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2277.2239, device='cuda:0', grad_fn=<MulBackward0>)}
[123:29759]  loss_total_vae=10149.452  loss_recon=6548.105  loss_kld=849.326  loss_prediction=345.355  loss_true_values=2406.666  loss_total_vae_epoch=10190.994  
tracking changes
Validation stop evaluation
29759 123
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
94    95.0  6680.684542  828.337510  0.360112  0.922665  0.833049
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896

[124 rows x 6 columns]
Now val counter at: 1
Latent 0.3606146603822708   0.358158301975992
BCE 1.087733006477356 1.0334578421380785
Losses: {'total_vae': tensor(10330.0908, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6732.9561, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(856.0659, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(374.3114, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2366.7571, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
29999 124
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
95    96.0  6679.083438  825.909776  0.358907  0.895086  0.836533
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457

[125 rows x 6 columns]
Latent 0.35832796692848207   0.3569952093892627
BCE 1.048428295056025 1.0295433276229435
Losses: {'total_vae': tensor(10114.2314, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6612.3730, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(819.7203, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(303.2236, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2378.9146, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
30239 125
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
96    97.0  6679.452646  817.695233  0.359709  0.889434  0.833368
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806

[126 rows x 6 columns]
Now val counter at: 2
Latent 0.358807235956192   0.3593840767939886
BCE 1.0613256851832071 1.048014059331682
Saved model at epoch 126
Losses: {'total_vae': tensor(10208.5391, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6763.8828, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(870.7325, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(314.8755, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2259.0483, device='cuda:0', grad_fn=<MulBackward0>)}
[126:30479]  loss_total_vae=10244.189  loss_recon=6656.956  loss_kld=852.288  loss_prediction=369.814  loss_true_values=2365.131  loss_total_vae_epoch=10170.813  
tracking changes
Validation stop evaluation
30479 126
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
97    98.0  6690.765771  772.269674  0.365738  0.919301  0.832793
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228

[127 rows x 6 columns]
Now val counter at: 1
Latent 0.3604256590207418   0.35856541163391537
BCE 1.0906568586826324 1.0520314223236509
Losses: {'total_vae': tensor(10151.1719, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6669.4424, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(839.3088, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(341.9352, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2300.4858, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
30719 127
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
98    99.0  6680.689875  804.526436  0.362104  0.920695  0.831185
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496

[128 rows x 6 columns]
Latent 0.3577780465284983   0.35924995442231494
BCE 1.0861138125260672 1.065828995572196
Saved model at epoch 128
Losses: {'total_vae': tensor(10184.2080, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6734.0059, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(859.7049, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(284.3013, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2306.1956, device='cuda:0', grad_fn=<MulBackward0>)}
[128:30959]  loss_total_vae=10189.677  loss_recon=6557.703  loss_kld=875.073  loss_prediction=385.035  loss_true_values=2371.865  loss_total_vae_epoch=10163.763  
tracking changes
Validation stop evaluation
30959 128
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
99   100.0  6678.369646  832.635065  0.358057  0.921516  0.834441
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868

[129 rows x 6 columns]
Now val counter at: 1
Latent 0.3596369793017705   0.35918695396847194
BCE 1.0795554975668589 1.066803612973955
Losses: {'total_vae': tensor(10091.5381, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6612.4180, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(835.3336, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(317.9768, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2325.8093, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
31199 129
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
100  101.0  6677.078521  827.214217  0.359345  0.912740  0.834282
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399

[130 rows x 6 columns]
Now val counter at: 2
Latent 0.36562984784444175   0.3590036471684774
BCE 1.0857026120026907 1.0793654521306355
Losses: {'total_vae': tensor(10324.4531, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6753.1270, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(798.6459, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(327.7577, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2444.9224, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
31439 130
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
101  102.0  6673.753625  854.417470  0.355675  0.915535  0.835879
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077

[131 rows x 6 columns]
Now val counter at: 3
Latent 0.3604279865821203   0.35928022828367023
BCE 1.1035203158855438 1.0854420562585194
Losses: {'total_vae': tensor(9960.5479, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6589.7148, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(852.0881, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(239.5045, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2279.2407, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
31679 131
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
102  103.0  6672.414563  832.787168  0.359258  0.925650  0.833686
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043

[132 rows x 6 columns]
Now val counter at: 4
Latent 0.36168851554393766   0.3610149578915702
BCE 1.101698233683904 1.083790640698539
Losses: {'total_vae': tensor(9947.9365, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6471.7334, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(834.3777, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(310.3623, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2331.4626, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
31919 132
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
103  104.0  6675.901188  818.649516  0.360358  0.929766  0.833680
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210

[133 rows x 6 columns]
Latent 0.36098471681276956   0.36189827124277746
BCE 1.1010643243789673 1.0895928084850313
Saved model at epoch 133
Losses: {'total_vae': tensor(10182.6055, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6637.7773, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(835.1550, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(369.9710, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2339.7014, device='cuda:0', grad_fn=<MulBackward0>)}
[133:32159]  loss_total_vae=10162.957  loss_recon=6684.796  loss_kld=838.313  loss_prediction=256.272  loss_true_values=2383.576  loss_total_vae_epoch=10123.590  
tracking changes
Validation stop evaluation
32159 133
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
104  105.0  6678.179062  823.724611  0.359232  0.942110  0.834849
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168

[134 rows x 6 columns]
Now val counter at: 1
Latent 0.35839015046755474   0.36258211665683326
BCE 1.1084144632021586 1.0969737205240462
Saved model at epoch 134
Losses: {'total_vae': tensor(10095.6055, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6581.1899, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(867.2048, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(297.2608, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2349.9507, device='cuda:0', grad_fn=<MulBackward0>)}
[134:32399]  loss_total_vae=10155.939  loss_recon=6668.650  loss_kld=868.850  loss_prediction=301.294  loss_true_values=2317.146  loss_total_vae_epoch=10117.329  
tracking changes
Validation stop evaluation
32399 134
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
105  106.0  6678.564021  833.514740  0.358456  0.939693  0.836901
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649

[135 rows x 6 columns]
Now val counter at: 1
Latent 0.35871914625167844   0.3610337396462759
BCE 1.1281126936276753 1.1020942913161385
Saved model at epoch 135
Losses: {'total_vae': tensor(10172.9707, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6661.6807, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(860.8939, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(296.7126, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2353.6836, device='cuda:0', grad_fn=<MulBackward0>)}
[135:32639]  loss_total_vae=10141.794  loss_recon=6739.652  loss_kld=877.667  loss_prediction=208.702  loss_true_values=2315.773  loss_total_vae_epoch=10115.833  
tracking changes
Validation stop evaluation
32639 135
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
106  107.0  6677.017604  798.305408  0.363652  0.946125  0.835585
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852

[136 rows x 6 columns]
Now val counter at: 1
Latent 0.35891383488972983   0.36035446094142065
BCE 1.1253134747346243 1.10372567375501
Saved model at epoch 136
Losses: {'total_vae': tensor(10089.2090, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6667.0454, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(854.3760, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(255.0267, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2312.7605, device='cuda:0', grad_fn=<MulBackward0>)}
[136:32879]  loss_total_vae=10078.414  loss_recon=6618.273  loss_kld=824.833  loss_prediction=266.957  loss_true_values=2368.350  loss_total_vae_epoch=10105.323  
tracking changes
Validation stop evaluation
32879 136
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
107  108.0  6669.348583  857.349373  0.356177  0.962444  0.833318
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856

[137 rows x 6 columns]
Now val counter at: 1
Latent 0.3594316740830739   0.3593646711773342
BCE 1.1551198601722716 1.1125304937362672
Losses: {'total_vae': tensor(10029.6494, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6594.9565, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(844.5239, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(258.9327, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2331.2361, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
33119 137
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
108  109.0  6670.150000  841.154681  0.357510  0.959163  0.833530
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312

[138 rows x 6 columns]
Now val counter at: 2
Latent 0.3624985347191493   0.35867437720298767
BCE 1.1508286078770955 1.1206135438548195
Losses: {'total_vae': tensor(10336.9746, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6818.1279, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(835.7733, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(223.7467, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2459.3262, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
33359 138
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
109  110.0  6674.107208  845.961440  0.357577  0.964568  0.834644
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486

[139 rows x 6 columns]
Latent 0.361358650525411   0.3590215517414941
BCE 1.1320088903109233 1.1361820095115236
Saved model at epoch 139
Losses: {'total_vae': tensor(10169.9238, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6767.5068, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(824.4559, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(242.9792, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2334.9817, device='cuda:0', grad_fn=<MulBackward0>)}
[139:33599]  loss_total_vae=10029.381  loss_recon=6582.480  loss_kld=868.028  loss_prediction=260.986  loss_true_values=2317.886  loss_total_vae_epoch=10086.159  
tracking changes
Validation stop evaluation
33599 139
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
110  111.0  6677.024521  833.244741  0.360144  0.975724  0.835578
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711

[140 rows x 6 columns]
Now val counter at: 1
Latent 0.3633065273364385   0.3602813478973177
BCE 1.163887478907903 1.143753980927997
Losses: {'total_vae': tensor(9834.4883, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6506.2759, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(832.1509, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(164.9982, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2331.0640, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
33839 140
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
111  112.0  6680.030333  811.621769  0.363033  0.996309  0.835153
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233

[141 rows x 6 columns]
Now val counter at: 2
Latent 0.36236652731895447   0.3610962864425447
BCE 1.1701442758242289 1.1459857861200968
Losses: {'total_vae': tensor(10185.4326, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6763.3306, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(837.3563, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(247.9108, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2336.8350, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
34079 141
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
112  113.0  6671.491688  848.246859  0.358577  0.986732  0.836985
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730

[142 rows x 6 columns]
Now val counter at: 3
Latent 0.3568000664313634   0.36238790419366623
BCE 1.1782498101393382 1.1489083256986405
Saved model at epoch 142
Losses: {'total_vae': tensor(9969.0977, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6597.8291, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(852.9365, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(171.4609, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2346.8706, device='cuda:0', grad_fn=<MulBackward0>)}
[142:34319]  loss_total_vae=10091.122  loss_recon=6601.006  loss_kld=843.385  loss_prediction=248.333  loss_true_values=2398.398  loss_total_vae_epoch=10067.670  
tracking changes
Validation stop evaluation
34319 142
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
113  114.0  6672.696604  839.142275  0.359129  0.981275  0.836674
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249

[143 rows x 6 columns]
Now val counter at: 1
Latent 0.36276839474836986   0.3623439017269347
BCE 1.169308751821518 1.1553468816810184
Losses: {'total_vae': tensor(10026.7559, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6669.2812, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(814.6990, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(190.8464, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2351.9287, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
34559 143
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
114  115.0  6675.677292  824.091001  0.360522  0.984119  0.837853
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177

[144 rows x 6 columns]
Now val counter at: 2
Latent 0.3583661158879598   0.36082437369558545
BCE 1.183470074335734 1.1707605216238235
Saved model at epoch 144
Losses: {'total_vae': tensor(10112.4512, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6600.6362, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(862.9662, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(269.4976, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2379.3516, device='cuda:0', grad_fn=<MulBackward0>)}
[144:34799]  loss_total_vae=10246.015  loss_recon=6767.410  loss_kld=870.507  loss_prediction=261.493  loss_true_values=2346.605  loss_total_vae_epoch=10054.954  
tracking changes
Validation stop evaluation
34799 144
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
115  116.0  6673.700146  839.561776  0.359608  0.998558  0.837746
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630

[145 rows x 6 columns]
Now val counter at: 1
Latent 0.36052761574586234   0.36064499616622925
BCE 1.22386687596639 1.1725676125950282
Saved model at epoch 145
Losses: {'total_vae': tensor(10266.6328, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6718.5996, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(846.9917, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(324.9981, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2376.0432, device='cuda:0', grad_fn=<MulBackward0>)}
[145:35039]  loss_total_vae=10131.928  loss_recon=6773.539  loss_kld=863.051  loss_prediction=177.090  loss_true_values=2318.248  loss_total_vae_epoch=10052.515  
tracking changes
Validation stop evaluation
35039 145
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
116  117.0  6670.729375  837.040725  0.359317  0.991687  0.838258
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914

[146 rows x 6 columns]
Saved model at epoch 146
Losses: {'total_vae': tensor(10066.5879, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6728.8369, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(836.4778, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(163.2428, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2338.0308, device='cuda:0', grad_fn=<MulBackward0>)}
[146:35279]  loss_total_vae=10247.246  loss_recon=6773.445  loss_kld=842.460  loss_prediction=296.481  loss_true_values=2334.860  loss_total_vae_epoch=10044.112  
tracking changes
Validation stop evaluation
35279 146
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
117  118.0  6680.049333  820.585246  0.360917  1.007096  0.839033
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252

[147 rows x 6 columns]
Now val counter at: 1
Latent 0.3620213290055593   0.36055404212739733
BCE 1.2072579125563303 1.192215234041214
Losses: {'total_vae': tensor(10099.5352, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6600.7900, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(817.0129, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(298.2878, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2383.4446, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
35519 147
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
118  119.0  6682.651375  803.349408  0.364504  1.039368  0.837082
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737

[148 rows x 6 columns]
Latent 0.3582481950521469   0.3596390773852667
BCE 1.1761096994082132 1.2012215428882176
Saved model at epoch 148
Losses: {'total_vae': tensor(10049.6484, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6601.9902, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(869.4031, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(227.7187, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2350.5366, device='cuda:0', grad_fn=<MulBackward0>)}
[148:35759]  loss_total_vae=10175.990  loss_recon=6815.007  loss_kld=810.687  loss_prediction=236.196  loss_true_values=2314.100  loss_total_vae_epoch=10030.337  
tracking changes
Validation stop evaluation
35759 148
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
119  120.0  6669.441375  844.343040  0.360243  1.031677  0.840163
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793

[149 rows x 6 columns]
Now val counter at: 1
Latent 0.3568263401587804   0.3608574817577998
BCE 1.2278961579004923 1.2091508222950829
Saved model at epoch 149
Losses: {'total_vae': tensor(9950.1562, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6673.2549, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(856.8755, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(155.1250, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2264.9011, device='cuda:0', grad_fn=<MulBackward0>)}
[149:35999]  loss_total_vae=10118.492  loss_recon=6541.646  loss_kld=838.966  loss_prediction=309.421  loss_true_values=2428.460  loss_total_vae_epoch=10023.032  
tracking changes
Validation stop evaluation
35999 149
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
120  121.0  6657.593479  898.958185  0.353448  1.032321  0.836668
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311

[150 rows x 6 columns]
Now val counter at: 1
Latent 0.36493607560793556   0.3600976748598946
BCE 1.2255121409893035 1.1932317634423573
Losses: {'total_vae': tensor(9976.6270, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6653.1543, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(795.6207, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(175.7224, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2352.1292, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
36239 150
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
121  122.0  6678.420938  826.291962  0.360784  1.036376  0.837787
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158

[151 rows x 6 columns]
Now val counter at: 2
Latent 0.3629439483086268   0.3590319547388288
BCE 1.2436788956324258 1.203754589955012
Losses: {'total_vae': tensor(9983.5117, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6655.1128, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(834.6453, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(171.0051, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2322.7493, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
36479 151
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
122  123.0  6663.143042  864.673088  0.356754  1.019933  0.838863
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585

[152 rows x 6 columns]
Latent 0.3588194578886032   0.360003536939621
BCE 1.234408829609553 1.209839332766003
Saved model at epoch 152
Losses: {'total_vae': tensor(9963.4902, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6624.7236, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(870.3099, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(203.8520, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2264.6045, device='cuda:0', grad_fn=<MulBackward0>)}
[152:36719]  loss_total_vae=9991.079  loss_recon=6599.682  loss_kld=826.106  loss_prediction=194.367  loss_true_values=2370.925  loss_total_vae_epoch=10015.890  
tracking changes
Validation stop evaluation
36719 152
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
123  124.0  6668.639042  852.467859  0.360615  1.087733  0.830896
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844

[153 rows x 6 columns]
Now val counter at: 1
Latent 0.36289428969224297   0.3615687880251142
BCE 1.2523783683776855 1.2323623981740741
Losses: {'total_vae': tensor(10147.2100, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6719.9106, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(834.2892, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(218.7712, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2374.2393, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
36959 153
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
124  125.0  6668.523000  854.486483  0.358328  1.048428  0.838457
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659

[154 rows x 6 columns]
Latent 0.36135024229685464   0.3622331606017219
BCE 1.2412669360637665 1.2345332887437606
Saved model at epoch 154
Losses: {'total_vae': tensor(10184.9521, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6813.3159, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(848.3001, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(185.3344, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2338.0017, device='cuda:0', grad_fn=<MulBackward0>)}
[154:37199]  loss_total_vae=9983.166  loss_recon=6635.348  loss_kld=845.421  loss_prediction=188.366  loss_true_values=2314.031  loss_total_vae_epoch=9997.247  
tracking changes
Validation stop evaluation
37199 154
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
125  126.0  6670.082396  854.967212  0.358807  1.061326  0.837806
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033

[155 rows x 6 columns]
Now val counter at: 1
Latent 0.3593350867430369   0.361552565296491
BCE 1.2778133809566499 1.2434886978732214
Saved model at epoch 155
Losses: {'total_vae': tensor(9834.0889, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6524.7905, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(857.7720, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(121.5731, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2329.9534, device='cuda:0', grad_fn=<MulBackward0>)}
[155:37439]  loss_total_vae=10029.749  loss_recon=6637.072  loss_kld=843.745  loss_prediction=196.396  loss_true_values=2352.535  loss_total_vae_epoch=9987.823  
tracking changes
Validation stop evaluation
37439 155
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
126  127.0  6670.425063  837.786353  0.360426  1.090657  0.832228
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670

[156 rows x 6 columns]
Now val counter at: 1
Latent 0.3628392418225606   0.3610213299592336
BCE 1.2809136052926382 1.242684711350335
Losses: {'total_vae': tensor(10004.1045, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6687.7856, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(808.0126, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(180.4088, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2327.8972, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
37679 156
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
127  128.0  6670.007208  864.917338  0.357778  1.086114  0.836496
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252

[157 rows x 6 columns]
Now val counter at: 2
Latent 0.3615795979897181   0.36119320624404483
BCE 1.2958447178204855 1.2571528951327007
Losses: {'total_vae': tensor(10063.8691, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6708.7300, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(847.3229, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(151.5938, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2356.2229, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
37919 157
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
128  129.0  6666.807187  847.324854  0.359637  1.079555  0.839868
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656

[158 rows x 6 columns]
Latent 0.3606235146522522   0.3611748569541507
BCE 1.2941176255544027 1.2666646407710183
Saved model at epoch 158
Losses: {'total_vae': tensor(9915.0459, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6568.2388, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(880.1279, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(213.4835, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2253.1958, device='cuda:0', grad_fn=<MulBackward0>)}
[158:38159]  loss_total_vae=9919.197  loss_recon=6637.881  loss_kld=896.494  loss_prediction=143.819  loss_true_values=2241.003  loss_total_vae_epoch=9978.679  
tracking changes
Validation stop evaluation
38159 158
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
129  130.0  6687.126833  795.050903  0.365630  1.085703  0.839399
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904

[159 rows x 6 columns]
Now val counter at: 1
Latent 0.3572146306435267   0.36125130885177187
BCE 1.306568165620168 1.2848572346899245
Saved model at epoch 159
Losses: {'total_vae': tensor(10180.4326, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6800.6880, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(877.5610, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(158.4433, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2343.7405, device='cuda:0', grad_fn=<MulBackward0>)}
[159:38399]  loss_total_vae=9974.411  loss_recon=6713.273  loss_kld=870.553  loss_prediction=147.189  loss_true_values=2243.396  loss_total_vae_epoch=9970.845  
tracking changes
Validation stop evaluation
38399 159
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
130  131.0  6672.055312  834.216496  0.360428  1.103520  0.836077
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118

[160 rows x 6 columns]
Now val counter at: 1
Latent 0.36048503617445626   0.3616807848215103
BCE 1.3162347733974458 1.2902919828891755
Saved model at epoch 160
Losses: {'total_vae': tensor(10077.5527, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6761.2744, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(860.7162, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(151.0780, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2304.4834, device='cuda:0', grad_fn=<MulBackward0>)}
[160:38639]  loss_total_vae=10049.773  loss_recon=6757.286  loss_kld=851.639  loss_prediction=171.226  loss_true_values=2269.624  loss_total_vae_epoch=9967.167  
tracking changes
Validation stop evaluation
38639 160
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
131  132.0  6670.554271  827.916311  0.361689  1.101698  0.836043
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805

[161 rows x 6 columns]
Now val counter at: 1
Latent 0.3583201547463735   0.359805914428499
BCE 1.3222015937169393 1.298843502998352
Saved model at epoch 161
Losses: {'total_vae': tensor(9917.5928, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6640.1274, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(885.6162, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(188.0353, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2203.8142, device='cuda:0', grad_fn=<MulBackward0>)}
[161:38879]  loss_total_vae=9797.713  loss_recon=6534.631  loss_kld=865.321  loss_prediction=140.669  loss_true_values=2257.092  loss_total_vae_epoch=9960.799  
tracking changes
Validation stop evaluation
38879 161
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
132  133.0  6678.937937  826.660510  0.360985  1.101064  0.842210
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759

[162 rows x 6 columns]
Now val counter at: 1
Latent 0.3584138333797455   0.3594410604900784
BCE 1.3277430454889934 1.305640188190672
Saved model at epoch 162
Losses: {'total_vae': tensor(9787.7441, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6563.6079, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(885.7380, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(109.1645, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2229.2341, device='cuda:0', grad_fn=<MulBackward0>)}
[162:39119]  loss_total_vae=9923.479  loss_recon=6589.308  loss_kld=864.396  loss_prediction=168.610  loss_true_values=2301.164  loss_total_vae_epoch=9958.478  
tracking changes
Validation stop evaluation
39119 162
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
133  134.0  6665.196229  859.225389  0.358390  1.108414  0.838168
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171

[163 rows x 6 columns]
Now val counter at: 1
Latent 0.35872214833895366   0.3586732738547855
BCE 1.3532513936360677 1.3150015109115178
Losses: {'total_vae': tensor(9927.4775, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6701.1924, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(874.3061, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(122.7966, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2229.1826, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
39359 163
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
134  135.0  6662.157271  862.264028  0.358719  1.128113  0.838649
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465

[164 rows x 6 columns]
Now val counter at: 2
Latent 0.35877127250035606   0.35907300810019177
BCE 1.3596201380093893 1.3220598042011262
Saved model at epoch 164
Losses: {'total_vae': tensor(9920.5986, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6653.5522, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(893.5936, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(103.0939, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2270.3591, device='cuda:0', grad_fn=<MulBackward0>)}
[164:39599]  loss_total_vae=9861.424  loss_recon=6593.748  loss_kld=852.696  loss_prediction=101.078  loss_true_values=2313.902  loss_total_vae_epoch=9948.064  
tracking changes
Validation stop evaluation
39599 164
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
135  136.0  6667.384458  860.101784  0.358914  1.125313  0.836852
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121

[165 rows x 6 columns]
Now val counter at: 1
Latent 0.359316090742747   0.35848537882169085
BCE 1.33427241841952 1.334398677614
Saved model at epoch 165
Losses: {'total_vae': tensor(9868.2178, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6696.2944, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(873.0280, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(103.6284, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2195.2671, device='cuda:0', grad_fn=<MulBackward0>)}
[165:39839]  loss_total_vae=10013.844  loss_recon=6632.989  loss_kld=818.248  loss_prediction=178.801  loss_true_values=2383.806  loss_total_vae_epoch=9944.094  
tracking changes
Validation stop evaluation
39839 165
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
136  137.0  6676.592604  857.620709  0.359432  1.155120  0.836856
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458

[166 rows x 6 columns]
Now val counter at: 1
Latent 0.3672063489754995   0.3586357514063517
BCE 1.3398244897524516 1.3468715257114834
Saved model at epoch 166
Losses: {'total_vae': tensor(9944.5098, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6639.9263, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(786.2778, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(127.9615, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2390.3438, device='cuda:0', grad_fn=<MulBackward0>)}
[166:40079]  loss_total_vae=9902.231  loss_recon=6642.722  loss_kld=881.125  loss_prediction=138.371  loss_true_values=2240.014  loss_total_vae_epoch=9935.121  
tracking changes
Validation stop evaluation
40079 166
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
137  138.0  6676.052208  827.990401  0.362499  1.150829  0.839312
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609

[167 rows x 6 columns]
Now val counter at: 1
Latent 0.35855843921502434   0.3589365038606856
BCE 1.3549981753031413 1.3490479833549924
Saved model at epoch 167
Losses: {'total_vae': tensor(9804.6182, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6543.3242, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(870.2021, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(147.4215, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2243.6702, device='cuda:0', grad_fn=<MulBackward0>)}
[167:40319]  loss_total_vae=9932.146  loss_recon=6603.925  loss_kld=855.564  loss_prediction=97.381  loss_true_values=2375.276  loss_total_vae_epoch=9927.019  
tracking changes
Validation stop evaluation
40319 167
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
138  139.0  6668.668500  836.586757  0.361359  1.132009  0.838486
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298

[168 rows x 6 columns]
Now val counter at: 1
Latent 0.3588258216778437   0.3617645707395342
BCE 1.3791513601938883 1.3445723487271202
Saved model at epoch 168
Losses: {'total_vae': tensor(9820.1836, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6535.9116, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(881.4735, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(135.3377, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2267.4607, device='cuda:0', grad_fn=<MulBackward0>)}
[168:40559]  loss_total_vae=10004.812  loss_recon=6715.127  loss_kld=852.788  loss_prediction=98.438  loss_true_values=2338.459  loss_total_vae_epoch=9933.057  
tracking changes
Validation stop evaluation
40559 168
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
139  140.0  6674.393604  823.453072  0.363307  1.163887  0.842711
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145

[169 rows x 6 columns]
Now val counter at: 1
Latent 0.3601452042659124   0.3616936263110903
BCE 1.37841135263443 1.3430316944917042
Saved model at epoch 169
Losses: {'total_vae': tensor(9928.3340, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6643.6191, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(861.9140, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(151.5771, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2271.2241, device='cuda:0', grad_fn=<MulBackward0>)}
[169:40799]  loss_total_vae=9936.280  loss_recon=6593.662  loss_kld=859.750  loss_prediction=94.866  loss_true_values=2388.002  loss_total_vae_epoch=9920.351  
tracking changes
Validation stop evaluation
40799 169
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
140  141.0  6674.539458  832.281425  0.362367  1.170144  0.840233
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827

[170 rows x 6 columns]
Now val counter at: 1
Latent 0.3602526863416036   0.36153020328945584
BCE 1.3844651142756144 1.357991341749827
Saved model at epoch 170
Losses: {'total_vae': tensor(9864.9580, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6656.7690, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(851.5384, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(77.1754, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2279.4751, device='cuda:0', grad_fn=<MulBackward0>)}
[170:41039]  loss_total_vae=9896.621  loss_recon=6634.373  loss_kld=860.221  loss_prediction=137.093  loss_true_values=2264.934  loss_total_vae_epoch=9914.842  
tracking changes
Validation stop evaluation
41039 170
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
141  142.0  6662.250479  873.774864  0.356800  1.178250  0.837730
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474

[171 rows x 6 columns]
Now val counter at: 1
Latent 0.36081039408842724   0.3591764883862601
BCE 1.4002269188563028 1.3708536293771532
Losses: {'total_vae': tensor(9963.8652, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6707.1011, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(836.2717, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(178.2936, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2242.1992, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
41279 171
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
142  143.0  6676.715458  819.860954  0.362768  1.169309  0.838249
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210

[172 rows x 6 columns]
Now val counter at: 2
Latent 0.36249256630738574   0.3597412374284532
BCE 1.4080723961194357 1.3806759423679775
Losses: {'total_vae': tensor(9990.8789, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6674.2168, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(833.7915, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(132.1297, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2350.7402, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
41519 172
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
143  144.0  6661.322333  863.532430  0.358366  1.183470  0.839177
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244

[173 rows x 6 columns]
Now val counter at: 3
Latent 0.3629670312007268   0.3604027615653144
BCE 1.4539531946182251 1.3877011285887824
Losses: {'total_vae': tensor(9787.2852, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6602.5850, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(826.7017, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(74.1788, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2283.8203, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
41759 173
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
144  145.0  6673.365521  848.751961  0.360528  1.223867  0.835630
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445

[174 rows x 6 columns]
Latent 0.36086352268854777   0.36118521557913885
BCE 1.41503218015035 1.3975881430837844
Saved model at epoch 174
Losses: {'total_vae': tensor(9892.2334, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6628.9961, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(862.4180, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(153.5088, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2247.3105, device='cuda:0', grad_fn=<MulBackward0>)}
[174:41999]  loss_total_vae=9959.593  loss_recon=6733.755  loss_kld=847.425  loss_prediction=94.832  loss_true_values=2283.580  loss_total_vae_epoch=9896.188  
tracking changes
Validation stop evaluation
41999 174
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
145  146.0  6672.740896  843.120150  0.360024  1.196328  0.838914
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389

[175 rows x 6 columns]
Now val counter at: 1
Latent 0.3618661493062973   0.36208999719884655
BCE 1.4166516502698263 1.4207508365313213
Saved model at epoch 175
Losses: {'total_vae': tensor(9673.6885, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6491.8804, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(841.8584, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(99.4346, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2240.5149, device='cuda:0', grad_fn=<MulBackward0>)}
[175:42239]  loss_total_vae=9819.582  loss_recon=6637.082  loss_kld=867.419  loss_prediction=101.894  loss_true_values=2213.187  loss_total_vae_epoch=9892.646  
tracking changes
Validation stop evaluation
42239 175
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
146  147.0  6672.542250  832.432371  0.362021  1.207258  0.842252
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171

[176 rows x 6 columns]
Now val counter at: 1
Latent 0.35934453308582304   0.3621077067322201
BCE 1.441440486907959 1.4256859236293369
Saved model at epoch 176
Losses: {'total_vae': tensor(10060.2910, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6714.8384, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(872.2350, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(106.3617, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2366.8564, device='cuda:0', grad_fn=<MulBackward0>)}
[176:42479]  loss_total_vae=10045.062  loss_recon=6771.723  loss_kld=865.056  loss_prediction=114.867  loss_true_values=2293.417  loss_total_vae_epoch=9886.616  
tracking changes
Validation stop evaluation
42479 176
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
147  148.0  6667.362625  859.148222  0.358248  1.176110  0.843737
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029

[177 rows x 6 columns]
Now val counter at: 1
Latent 0.3617153108119965   0.3618989010651907
BCE 1.4350730141003927 1.4285456750128003
Saved model at epoch 177
Losses: {'total_vae': tensor(9936.3564, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6732.8115, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(877.7177, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(115.3037, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2210.5234, device='cuda:0', grad_fn=<MulBackward0>)}
[177:42719]  loss_total_vae=9940.171  loss_recon=6622.108  loss_kld=853.806  loss_prediction=189.153  loss_true_values=2275.104  loss_total_vae_epoch=9888.125  
tracking changes
Validation stop evaluation
42719 177
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
148  149.0  6668.718167  876.667137  0.356826  1.227896  0.837793
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471

[178 rows x 6 columns]
Now val counter at: 1
Latent 0.36203904350598654   0.360691401693556
BCE 1.4635913530985514 1.4243747724427116
Losses: {'total_vae': tensor(9815.3789, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6570.5967, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(852.2993, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(116.9162, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2275.5664, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
42959 178
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
149  150.0  6677.282396  817.375358  0.364936  1.225512  0.842311
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142

[179 rows x 6 columns]
Now val counter at: 2
Latent 0.362917352716128   0.36097533106803886
BCE 1.469567318757375 1.4310550504260593
Losses: {'total_vae': tensor(9917.9629, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6653.5654, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(856.4819, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(75.0088, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2332.9067, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
43199 179
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
150  151.0  6669.022063  832.919875  0.362944  1.243679  0.841158
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372

[180 rows x 6 columns]
Latent 0.36092512210210165   0.36103296246793537
BCE 1.4435999731222788 1.4467016180356342
Saved model at epoch 180
Losses: {'total_vae': tensor(9750.3955, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6529.1460, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(877.9897, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(74.1861, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2269.0735, device='cuda:0', grad_fn=<MulBackward0>)}
[180:43439]  loss_total_vae=9728.618  loss_recon=6580.627  loss_kld=869.666  loss_prediction=77.312  loss_true_values=2201.013  loss_total_vae_epoch=9870.880  
tracking changes
Validation stop evaluation
43439 180
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
151  152.0  6662.311417  862.964581  0.358819  1.234409  0.838585
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146

[181 rows x 6 columns]
Now val counter at: 1
Latent 0.3602421989043554   0.36222390234470364
BCE 1.4912842710812886 1.4560772286521064
Saved model at epoch 181
Losses: {'total_vae': tensor(9883.1016, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6614.0425, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(867.5879, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(87.1683, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2314.3025, device='cuda:0', grad_fn=<MulBackward0>)}
[181:43679]  loss_total_vae=9839.631  loss_recon=6561.404  loss_kld=870.761  loss_prediction=118.490  loss_true_values=2288.976  loss_total_vae_epoch=9872.004  
tracking changes
Validation stop evaluation
43679 181
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
152  153.0  6672.466438  828.041876  0.362894  1.252378  0.841844
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157

[182 rows x 6 columns]
Saved model at epoch 182
Losses: {'total_vae': tensor(9863.6494, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6668.3311, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(889.9005, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(84.5923, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2220.8254, device='cuda:0', grad_fn=<MulBackward0>)}
[182:43919]  loss_total_vae=9903.907  loss_recon=6659.824  loss_kld=857.441  loss_prediction=102.234  loss_true_values=2284.408  loss_total_vae_epoch=9862.530  
tracking changes
Validation stop evaluation
43919 182
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
153  154.0  6671.083667  833.564164  0.361350  1.241267  0.840659
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662

[183 rows x 6 columns]
Now val counter at: 1
Latent 0.35889119406541187   0.3613615579075284
BCE 1.570452046394348 1.468150520986981
Saved model at epoch 183
Losses: {'total_vae': tensor(9785.6064, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6526.6519, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(902.1436, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(109.7052, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2247.1057, device='cuda:0', grad_fn=<MulBackward0>)}
[183:44159]  loss_total_vae=9798.418  loss_recon=6618.380  loss_kld=870.637  loss_prediction=68.921  loss_true_values=2240.479  loss_total_vae_epoch=9859.279  
tracking changes
Validation stop evaluation
44159 183
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
154  155.0  6665.529438  861.561686  0.359335  1.277813  0.841033
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683

[184 rows x 6 columns]
Saved model at epoch 184
Losses: {'total_vae': tensor(9841.3604, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6605.1753, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(856.8721, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(86.1730, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2293.1404, device='cuda:0', grad_fn=<MulBackward0>)}
[184:44399]  loss_total_vae=9934.341  loss_recon=6702.255  loss_kld=876.340  loss_prediction=73.776  loss_true_values=2281.970  loss_total_vae_epoch=9855.773  
tracking changes
Validation stop evaluation
44399 184
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
155  156.0  6664.942854  827.149790  0.362839  1.280914  0.839670
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071

[185 rows x 6 columns]
Now val counter at: 1
Latent 0.35969614883263906   0.35912382503350576
BCE 1.5116053144137065 1.5174704591433208
Saved model at epoch 185
Losses: {'total_vae': tensor(9858.0527, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6623.5088, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(864.7106, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(82.8878, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2286.9456, device='cuda:0', grad_fn=<MulBackward0>)}
[185:44639]  loss_total_vae=9855.173  loss_recon=6690.154  loss_kld=866.688  loss_prediction=120.248  loss_true_values=2178.083  loss_total_vae_epoch=9858.086  
tracking changes
Validation stop evaluation
44639 185
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
156  157.0  6669.830958  841.234562  0.361580  1.295845  0.839252
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273

[186 rows x 6 columns]
Now val counter at: 1
Latent 0.36723083357016245   0.3585112942589654
BCE 1.5305959045886994 1.5230586171150209
Losses: {'total_vae': tensor(9828.9004, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6586.6636, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(807.8928, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(99.0843, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2335.2598, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
44879 186
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
157  158.0  6668.687896  847.965865  0.360624  1.294118  0.838656
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144

[187 rows x 6 columns]
Latent 0.35778623819351196   0.3589973164929284
BCE 1.5034777025381725 1.530035368601481
Saved model at epoch 187
Losses: {'total_vae': tensor(9944.2979, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6730.1636, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(884.8588, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(56.4344, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2272.8406, device='cuda:0', grad_fn=<MulBackward0>)}
[187:45119]  loss_total_vae=9852.966  loss_recon=6602.458  loss_kld=823.824  loss_prediction=58.552  loss_true_values=2368.133  loss_total_vae_epoch=9846.383  
tracking changes
Validation stop evaluation
45119 187
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
158  159.0  6663.973792  884.598735  0.357215  1.306568  0.837904
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971

[188 rows x 6 columns]
Now val counter at: 1
Latent 0.36377686162789663   0.3617771963278453
BCE 1.5405523220698039 1.5167499879995983
Losses: {'total_vae': tensor(9912.5674, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6704.4443, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(835.9821, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(93.0650, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2279.0764, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
45359 188
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
159  160.0  6663.225708  854.941095  0.360485  1.316235  0.843118
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358

[189 rows x 6 columns]
Now val counter at: 2
Latent 0.360351832707723   0.3615710735321045
BCE 1.5439106186230978 1.5152263071801928
Saved model at epoch 189
Losses: {'total_vae': tensor(9791.4395, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6641.9028, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(851.7887, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(69.1267, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2228.6211, device='cuda:0', grad_fn=<MulBackward0>)}
[189:45599]  loss_total_vae=9970.842  loss_recon=6686.559  loss_kld=872.476  loss_prediction=105.999  loss_true_values=2305.808  loss_total_vae_epoch=9843.968  
tracking changes
Validation stop evaluation
45599 189
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
160  161.0  6668.078417  873.849514  0.358320  1.322202  0.841805
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424

[190 rows x 6 columns]
Now val counter at: 1
Latent 0.35742775599161786   0.3629313111305237
BCE 1.5535461584726968 1.5248753097322252
Saved model at epoch 190
Losses: {'total_vae': tensor(9795.2305, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6579.0259, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(900.6116, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(57.5541, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2258.0383, device='cuda:0', grad_fn=<MulBackward0>)}
[190:45839]  loss_total_vae=9983.199  loss_recon=6769.260  loss_kld=790.729  loss_prediction=74.595  loss_true_values=2348.616  loss_total_vae_epoch=9835.354  
tracking changes
Validation stop evaluation
45839 190
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
161  162.0  6669.420854  866.058598  0.358414  1.327743  0.840759
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352

[191 rows x 6 columns]
Now val counter at: 1
Latent 0.36269508600234984   0.3606383108430438
BCE 1.5520821253458659 1.5293135477436914
Losses: {'total_vae': tensor(9789.9541, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6626.9688, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(842.4338, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(55.6711, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2264.8809, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
46079 191
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
162  163.0  6661.319812  870.276632  0.358722  1.353251  0.838171
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288

[192 rows x 6 columns]
Now val counter at: 2
Latent 0.35771975020567576   0.3605188167757458
BCE 1.5894015709559122 1.5460030330551995
Saved model at epoch 192
Losses: {'total_vae': tensor(9830.1836, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6650.6143, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(855.5693, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(88.3569, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2235.6436, device='cuda:0', grad_fn=<MulBackward0>)}
[192:46319]  loss_total_vae=9965.734  loss_recon=6720.599  loss_kld=857.150  loss_prediction=94.980  loss_true_values=2293.006  loss_total_vae_epoch=9831.844  
tracking changes
Validation stop evaluation
46319 192
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
163  164.0  6660.286521  877.926152  0.358771  1.359620  0.839465
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075

[193 rows x 6 columns]
Now val counter at: 1
Latent 0.3589189529418945   0.36015822490056354
BCE 1.5587806582450867 1.549846300813887
Saved model at epoch 193
Losses: {'total_vae': tensor(9781.8926, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6553.9565, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(892.0246, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(60.5758, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2275.3362, device='cuda:0', grad_fn=<MulBackward0>)}
[193:46559]  loss_total_vae=9954.418  loss_recon=6725.336  loss_kld=839.489  loss_prediction=86.236  loss_true_values=2303.357  loss_total_vae_epoch=9820.786  
tracking changes
Validation stop evaluation
46559 193
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
164  165.0  6667.063583  858.836971  0.359316  1.334272  0.841121
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828

[194 rows x 6 columns]
Now val counter at: 1
Latent 0.3615749845902125   0.35928086406654786
BCE 1.6089910705884298 1.5650099515914917
Losses: {'total_vae': tensor(9885.8164, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6709.2085, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(854.3582, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(58.6317, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2263.6174, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
46799 194
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
165  166.0  6673.945208  799.996690  0.367206  1.339824  0.841458
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518

[195 rows x 6 columns]
Now val counter at: 2
Latent 0.36053439875443777   0.3597779297166401
BCE 1.6182943344116212 1.566754784848955
Losses: {'total_vae': tensor(9773.0635, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6643.2129, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(835.9955, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(80.3336, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2213.5212, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
47039 195
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
166  167.0  6673.473104  868.387018  0.358558  1.354998  0.840609
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830

[196 rows x 6 columns]
Now val counter at: 3
Latent 0.3624395926793416   0.35940456257926096
BCE 1.6169601281483967 1.5857244332631428
Losses: {'total_vae': tensor(9699.7510, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6527.9780, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(854.8641, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(43.2677, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2273.6414, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
47279 196
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
167  168.0  6657.905792  878.581394  0.358826  1.379151  0.839298
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509

[197 rows x 6 columns]
Latent 0.3567718674739202   0.36034277876218157
BCE 1.5958539923032125 1.5953553544150456
Saved model at epoch 197
Losses: {'total_vae': tensor(9835.4639, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6652.1665, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(889.9634, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(63.9673, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2229.3669, device='cuda:0', grad_fn=<MulBackward0>)}
[197:47519]  loss_total_vae=9911.444  loss_recon=6656.244  loss_kld=842.209  loss_prediction=73.267  loss_true_values=2339.725  loss_total_vae_epoch=9809.650  
tracking changes
Validation stop evaluation
47519 197
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
168  169.0  6666.977417  854.641779  0.360145  1.378411  0.840145
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843

[198 rows x 6 columns]
Now val counter at: 1
Latent 0.358135915795962   0.36151632534133066
BCE 1.6045735359191895 1.6147485110494826
Saved model at epoch 198
Losses: {'total_vae': tensor(9679.7109, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6502.4790, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(905.2450, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(61.4458, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2210.5405, device='cuda:0', grad_fn=<MulBackward0>)}
[198:47759]  loss_total_vae=9962.090  loss_recon=6738.925  loss_kld=841.890  loss_prediction=52.561  loss_true_values=2328.714  loss_total_vae_epoch=9809.861  
tracking changes
Validation stop evaluation
47759 198
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
169  170.0  6672.767396  851.769824  0.360253  1.384465  0.838827
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535

[199 rows x 6 columns]
Now val counter at: 1
Latent 0.36022230188051857   0.35991528630256653
BCE 1.6302340189615885 1.6103694849544101
Losses: {'total_vae': tensor(9691.7734, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6573.3511, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(867.9244, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(72.6739, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2177.8237, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
47999 199
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
170  171.0  6668.172729  845.691954  0.360810  1.400227  0.840474
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211

[200 rows x 6 columns]
Now val counter at: 2
Latent 0.36293517649173734   0.3591157919830746
BCE 1.6418186545372009 1.605795885456933
Losses: {'total_vae': tensor(9999.4883, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6748.5894, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(847.7204, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(142.1299, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2261.0486, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
48239 200
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
171  172.0  6671.184771  838.987282  0.362493  1.408072  0.841210
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688

[201 rows x 6 columns]
Latent 0.3613674620787303   0.35837669505013364
BCE 1.6278020858764648 1.610220515727997
Losses: {'total_vae': tensor(9845.7793, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6676.8174, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(835.4901, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(62.0455, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2271.4265, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
48479 201
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
172  173.0  6665.863542  842.947496  0.362967  1.453953  0.838244
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716

[202 rows x 6 columns]
Now val counter at: 3
Latent 0.36218328177928927   0.36043113138940597
BCE 1.652007782459259 1.625542069805993
Losses: {'total_vae': tensor(9816.3018, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6693.1113, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(847.6056, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(40.3514, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2235.2334, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
48719 202
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
173  174.0  6662.802938  852.645447  0.360864  1.415032  0.841445
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135

[203 rows x 6 columns]
Now val counter at: 4
Latent 0.36247940560181935   0.361508313483662
BCE 1.6498962004979452 1.6332849197917516
Losses: {'total_vae': tensor(9833.4180, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6700.3418, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(843.7275, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(54.4929, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2234.8555, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
48959 203
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
174  175.0  6664.740521  845.959747  0.361866  1.416652  0.842389
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534

[204 rows x 6 columns]
Latent 0.3609296401341756   0.362161973449919
BCE 1.6277160922686258 1.6405428409576415
Saved model at epoch 204
Losses: {'total_vae': tensor(9586.2305, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6457.4717, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(851.4362, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(79.2522, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2198.0701, device='cuda:0', grad_fn=<MulBackward0>)}
[204:49199]  loss_total_vae=9702.546  loss_recon=6492.686  loss_kld=836.401  loss_prediction=77.376  loss_true_values=2296.083  loss_total_vae_epoch=9794.874  
tracking changes
Validation stop evaluation
49199 204
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
175  176.0  6664.039083  866.287919  0.359345  1.441440  0.840171
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867

[205 rows x 6 columns]
Now val counter at: 1
Latent 0.3620565801858902   0.3620100498199463
BCE 1.672313137849172 1.6432353562778896
Losses: {'total_vae': tensor(9780.7422, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6623.2393, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(835.1453, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(70.7872, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2251.5703, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
49439 205
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
176  177.0  6665.281167  846.077039  0.361715  1.435073  0.842029
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958

[206 rows x 6 columns]
Latent 0.3598013579845428   0.36186410917176143
BCE 1.6345940073331198 1.6432066917419432
Saved model at epoch 206
Losses: {'total_vae': tensor(9902.9434, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6772.9868, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(862.7256, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(48.9607, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2218.2710, device='cuda:0', grad_fn=<MulBackward0>)}
[206:49679]  loss_total_vae=9781.777  loss_recon=6558.354  loss_kld=882.178  loss_prediction=86.450  loss_true_values=2254.796  loss_total_vae_epoch=9788.056  
tracking changes
Validation stop evaluation
49679 206
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
177  178.0  6671.358375  846.353737  0.362039  1.463591  0.842471
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481

[207 rows x 6 columns]
Now val counter at: 1
Latent 0.35897286931673683   0.36182187530729504
BCE 1.6890952905019125 1.6499751435385808
Saved model at epoch 207
Losses: {'total_vae': tensor(9704.5938, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6623.9087, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(862.2733, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(32.6253, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2185.7866, device='cuda:0', grad_fn=<MulBackward0>)}
[207:49919]  loss_total_vae=9782.773  loss_recon=6674.051  loss_kld=873.105  loss_prediction=60.344  loss_true_values=2175.273  loss_total_vae_epoch=9796.064  
tracking changes
Validation stop evaluation
49919 207
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
178  179.0  6671.827583  837.916034  0.362917  1.469567  0.841142
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584

[208 rows x 6 columns]
Now val counter at: 1
Latent 0.36201226512591045   0.36092919276820296
BCE 1.6923314929008484 1.6448744124836392
Losses: {'total_vae': tensor(9876.1787, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6743.0781, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(807.5590, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(26.9675, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2298.5742, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
50159 208
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
179  180.0  6664.174729  852.717407  0.360925  1.443600  0.844372
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787

[209 rows x 6 columns]
Now val counter at: 2
Latent 0.3606485495964686   0.3602769358290567
BCE 1.6971928000450134 1.665334145228068
Losses: {'total_vae': tensor(9876.7109, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6700.5059, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(837.8754, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(61.7197, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2276.6096, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
50399 209
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
180  181.0  6666.557667  862.670701  0.360242  1.491284  0.842146
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373

[210 rows x 6 columns]
Now val counter at: 3
Latent 0.3609026958545049   0.36026216414239665
BCE 1.6726877808570861 1.6720069302452936
Losses: {'total_vae': tensor(9850.9199, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6692.7617, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(866.1848, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(46.0252, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2245.9485, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
50639 210
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
181  182.0  6663.406542  877.486912  0.358238  1.490675  0.840157
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894

[211 rows x 6 columns]
Now val counter at: 4
Latent 0.36214041511217754   0.360544561346372
BCE 1.6689632296562196 1.6928731944825914
Saved model at epoch 211
Losses: {'total_vae': tensor(9760.6367, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6686.8350, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(864.3694, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(25.2706, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2184.1616, device='cuda:0', grad_fn=<MulBackward0>)}
[211:50879]  loss_total_vae=9700.469  loss_recon=6573.121  loss_kld=858.660  loss_prediction=44.349  loss_true_values=2224.340  loss_total_vae_epoch=9779.365  
tracking changes
Validation stop evaluation
50879 211
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
182  183.0  6659.005542  892.969716  0.358891  1.570452  0.832662
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994

[212 rows x 6 columns]
Now val counter at: 1
Latent 0.3620346665382385   0.36118783685896133
BCE 1.688912602265676 1.6874040246009827
Losses: {'total_vae': tensor(9548.0449, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6455.8438, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(826.4064, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(42.1884, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2223.6062, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
51119 212
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
183  184.0  6659.235875  883.286385  0.358405  1.508049  0.840683
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561

[213 rows x 6 columns]
Now val counter at: 2
Latent 0.36259022951126096   0.3612305535210503
BCE 1.7242217898368835 1.6796146035194397
Losses: {'total_vae': tensor(9717.9219, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6595.2979, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(853.7555, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(43.3257, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2225.5425, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
51359 213
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
184  185.0  6665.845604  859.438438  0.359696  1.511605  0.842071
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938

[214 rows x 6 columns]
Latent 0.35939841667811073   0.3616925925016403
BCE 1.7123075485229493 1.6768545375929937
Saved model at epoch 214
Losses: {'total_vae': tensor(9632.3281, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6538.6001, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(859.3669, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(41.2292, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2193.1323, device='cuda:0', grad_fn=<MulBackward0>)}
[214:51599]  loss_total_vae=9753.473  loss_recon=6675.514  loss_kld=851.400  loss_prediction=68.608  loss_true_values=2157.951  loss_total_vae_epoch=9765.056  
tracking changes
Validation stop evaluation
51599 214
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
185  186.0  6677.232042  801.102728  0.367231  1.530596  0.838273
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057

[215 rows x 6 columns]
Now val counter at: 1
Latent 0.36067869464556374   0.362255103720559
BCE 1.7268038749694825 1.6940325405862595
Saved model at epoch 215
Losses: {'total_vae': tensor(9734.8867, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6588.1230, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(853.6315, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(65.3495, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2227.7832, device='cuda:0', grad_fn=<MulBackward0>)}
[215:51839]  loss_total_vae=9777.064  loss_recon=6603.036  loss_kld=888.098  loss_prediction=62.636  loss_true_values=2223.296  loss_total_vae_epoch=9772.558  
tracking changes
Validation stop evaluation
51839 215
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
186  187.0  6665.376979  876.881215  0.357786  1.503478  0.844144
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503

[216 rows x 6 columns]
Now val counter at: 1
Latent 0.3589328736066818   0.36134110424253674
BCE 1.7511897047360738 1.7084806468751694
Saved model at epoch 216
Losses: {'total_vae': tensor(9700.9395, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6587.8286, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(874.7480, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(42.7915, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2195.5718, device='cuda:0', grad_fn=<MulBackward0>)}
[216:52079]  loss_total_vae=9733.956  loss_recon=6649.649  loss_kld=864.975  loss_prediction=53.894  loss_true_values=2165.437  loss_total_vae_epoch=9768.672  
tracking changes
Validation stop evaluation
52079 216
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
187  188.0  6665.445562  829.460132  0.363777  1.540552  0.840971
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845

[217 rows x 6 columns]
Now val counter at: 1
Latent 0.36127446790536244   0.36088911361164516
BCE 1.7449504375457763 1.7211110711097717
Losses: {'total_vae': tensor(9679.1074, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6591.2461, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(845.7828, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(31.4106, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2210.6680, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
52319 217
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
188  189.0  6664.010958  852.894271  0.360352  1.543911  0.841358
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948

[218 rows x 6 columns]
Now val counter at: 2
Latent 0.36195427775382993   0.3596699949767854
BCE 1.7354740381240845 1.7301003760761686
Losses: {'total_vae': tensor(9667.5996, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6581.3486, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(848.4197, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(33.9207, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2203.9104, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
52559 218
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
189  190.0  6663.424250  880.819930  0.357428  1.553546  0.843424
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496

[219 rows x 6 columns]
Now val counter at: 3
Latent 0.36015298863252004   0.3602953453858693
BCE 1.744019365310669 1.7409813390837776
Saved model at epoch 219
Losses: {'total_vae': tensor(9778.3301, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6635.9385, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(838.3721, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(41.9225, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2262.0977, device='cuda:0', grad_fn=<MulBackward0>)}
[219:52799]  loss_total_vae=9876.091  loss_recon=6714.021  loss_kld=851.632  loss_prediction=80.294  loss_true_values=2230.145  loss_total_vae_epoch=9752.919  
tracking changes
Validation stop evaluation
52799 219
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
190  191.0  6673.742417  838.535465  0.362695  1.552082  0.843352
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248

[220 rows x 6 columns]
Saved model at epoch 220
Losses: {'total_vae': tensor(9935.6895, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6755.2021, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(895.7813, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(42.7057, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2242., device='cuda:0', grad_fn=<MulBackward0>)}
[220:53039]  loss_total_vae=9780.799  loss_recon=6684.618  loss_kld=860.822  loss_prediction=40.387  loss_true_values=2194.971  loss_total_vae_epoch=9756.168  
tracking changes
Validation stop evaluation
53039 220
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
191  192.0  6661.024792  882.340605  0.357720  1.589402  0.843288
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209

[221 rows x 6 columns]
Now val counter at: 1
Latent 0.3595891376336416   0.3611272447639042
BCE 1.7433087825775146 1.7414812803268436
Saved model at epoch 221
Losses: {'total_vae': tensor(9751.9062, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6616.1787, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(834.3926, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(29.2646, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2272.0701, device='cuda:0', grad_fn=<MulBackward0>)}
[221:53279]  loss_total_vae=9994.620  loss_recon=6825.390  loss_kld=861.557  loss_prediction=42.180  loss_true_values=2265.493  loss_total_vae_epoch=9754.724  
tracking changes
Validation stop evaluation
53279 221
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
192  193.0  6663.191021  864.595866  0.358919  1.558781  0.841075
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520

[222 rows x 6 columns]
Now val counter at: 1
Latent 0.35964038372039797   0.3597745723194546
BCE 1.781334114074707 1.7350950850380793
Saved model at epoch 222
Losses: {'total_vae': tensor(9825.4111, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6749.3525, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(846.1005, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(38.7131, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2191.2449, device='cuda:0', grad_fn=<MulBackward0>)}
[222:53519]  loss_total_vae=9718.843  loss_recon=6659.746  loss_kld=831.682  loss_prediction=32.120  loss_true_values=2195.296  loss_total_vae_epoch=9753.316  
tracking changes
Validation stop evaluation
53519 222
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
193  194.0  6666.599187  855.094499  0.361575  1.608991  0.838828
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711

[223 rows x 6 columns]
Now val counter at: 1
Latent 0.36317817866802216   0.3589861922793918
BCE 1.773642603556315 1.7377066665225558
Losses: {'total_vae': tensor(9790.2129, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6654.7051, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(843.9147, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(39.2041, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2252.3892, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
53759 223
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
194  195.0  6664.053292  859.689648  0.360534  1.618294  0.840518
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328

[224 rows x 6 columns]
Now val counter at: 2
Latent 0.3631304562091827   0.35881532397535115
BCE 1.828497596581777 1.7501449161105687
Losses: {'total_vae': tensor(9735.3242, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6596.5674, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(841.6847, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(60.7021, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2236.3704, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
53999 224
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
195  196.0  6667.183833  839.241355  0.362440  1.616960  0.837830
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157

[225 rows x 6 columns]
Latent 0.3583262542883555   0.3608025666740206
BCE 1.7784271915753682 1.7660951667361788
Saved model at epoch 225
Losses: {'total_vae': tensor(9883.0596, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6724.0498, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(854.4095, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(56.0419, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2248.5581, device='cuda:0', grad_fn=<MulBackward0>)}
[225:54239]  loss_total_vae=9622.396  loss_recon=6552.617  loss_kld=868.623  loss_prediction=31.655  loss_true_values=2169.500  loss_total_vae_epoch=9741.656  
tracking changes
Validation stop evaluation
54239 225
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
196  197.0  6656.610938  893.948531  0.356772  1.595854  0.844509
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758

[226 rows x 6 columns]
Now val counter at: 1
Latent 0.3613258441289266   0.36198300619920093
BCE 1.798459736506144 1.7944914380709331
Saved model at epoch 226
Losses: {'total_vae': tensor(9736.7959, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6639.7061, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(826.1853, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(41.7559, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2229.1487, device='cuda:0', grad_fn=<MulBackward0>)}
[226:54479]  loss_total_vae=9688.683  loss_recon=6539.965  loss_kld=895.524  loss_prediction=53.278  loss_true_values=2199.916  loss_total_vae_epoch=9743.560  
tracking changes
Validation stop evaluation
54479 226
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
197  198.0  6659.984354  877.567515  0.358136  1.604574  0.843843
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657

[227 rows x 6 columns]
Now val counter at: 1
Latent 0.3599325974782308   0.3615449630551868
BCE 1.8444170872370402 1.7935224639044869
Saved model at epoch 227
Losses: {'total_vae': tensor(9792.8320, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6628.3667, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(862.7640, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(19.8706, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2281.8303, device='cuda:0', grad_fn=<MulBackward0>)}
[227:54719]  loss_total_vae=9762.521  loss_recon=6618.428  loss_kld=847.079  loss_prediction=23.919  loss_true_values=2273.096  loss_total_vae_epoch=9741.611  
tracking changes
Validation stop evaluation
54719 227
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
198  199.0  6664.269854  858.290861  0.360222  1.630234  0.841535
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974

[228 rows x 6 columns]
Now val counter at: 1
Latent 0.3608658492565155   0.36092751820882163
BCE 1.80805451075236 1.8017948415544298
Saved model at epoch 228
Losses: {'total_vae': tensor(9748.8652, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6604.3750, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(857.2925, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(43.6434, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2243.5544, device='cuda:0', grad_fn=<MulBackward0>)}
[228:54959]  loss_total_vae=9675.611  loss_recon=6582.132  loss_kld=867.166  loss_prediction=42.180  loss_true_values=2184.133  loss_total_vae_epoch=9739.054  
tracking changes
Validation stop evaluation
54959 228
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
199  200.0  6667.423146  844.309359  0.362935  1.641819  0.837211
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918

[229 rows x 6 columns]
Now val counter at: 1
Latent 0.36203556060791015   0.35986156529850427
BCE 1.827379318078359 1.8071013384395176
Losses: {'total_vae': tensor(9591.8809, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6549.1851, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(835.0158, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(49.0905, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2158.5896, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
55199 229
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
200  201.0  6664.207292  847.691156  0.361367  1.627802  0.839688
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036

[230 rows x 6 columns]
Latent 0.3597623288631439   0.3607080969545577
BCE 1.810427725315094 1.8169771114985147
Saved model at epoch 230
Losses: {'total_vae': tensor(9582.0205, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6469.0493, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(851.9980, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(40.0463, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2220.9270, device='cuda:0', grad_fn=<MulBackward0>)}
[230:55439]  loss_total_vae=9786.936  loss_recon=6651.820  loss_kld=872.015  loss_prediction=46.102  loss_true_values=2217.000  loss_total_vae_epoch=9735.691  
tracking changes
Validation stop evaluation
55439 230
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
201  202.0  6674.345917  840.672915  0.362183  1.652008  0.839716
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113

[231 rows x 6 columns]
Now val counter at: 1
Latent 0.36114476919174193   0.36094466911421885
BCE 1.821456257502238 1.8266169720225864
Saved model at epoch 231
Losses: {'total_vae': tensor(9702.0654, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6625.4204, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(870.5396, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(28.1748, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2177.9307, device='cuda:0', grad_fn=<MulBackward0>)}
[231:55679]  loss_total_vae=9650.623  loss_recon=6566.081  loss_kld=851.214  loss_prediction=34.688  loss_true_values=2198.641  loss_total_vae_epoch=9731.400  
tracking changes
Validation stop evaluation
55679 231
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
202  203.0  6664.252500  848.028975  0.362479  1.649896  0.839135
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432

[232 rows x 6 columns]
Now val counter at: 1
Latent 0.3614229748646418   0.3608879129091898
BCE 1.815200690428416 1.815287184715271
Saved model at epoch 232
Losses: {'total_vae': tensor(9751.0977, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6638.4150, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(860.9041, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(22.2741, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2229.5042, device='cuda:0', grad_fn=<MulBackward0>)}
[232:55919]  loss_total_vae=9733.434  loss_recon=6615.804  loss_kld=896.659  loss_prediction=61.010  loss_true_values=2159.960  loss_total_vae_epoch=9729.959  
tracking changes
Validation stop evaluation
55919 232
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
203  204.0  6667.008208  854.868160  0.360930  1.627716  0.841534
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715

[233 rows x 6 columns]
Saved model at epoch 233
Losses: {'total_vae': tensor(9842.2656, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6749.2354, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(889.5742, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(33.1219, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2170.3335, device='cuda:0', grad_fn=<MulBackward0>)}
[233:56159]  loss_total_vae=9708.352  loss_recon=6623.132  loss_kld=875.248  loss_prediction=71.419  loss_true_values=2138.553  loss_total_vae_epoch=9737.269  
tracking changes
Validation stop evaluation
56159 233
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
204  205.0  6667.347604  847.970353  0.362057  1.672313  0.838867
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291

[234 rows x 6 columns]
Now val counter at: 1
Latent 0.3586793452501297   0.36077669097317583
BCE 1.8367503960927327 1.8156948910819157
Saved model at epoch 234
Losses: {'total_vae': tensor(9759.9941, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6622.5181, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(881.6080, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(40.0815, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2215.7864, device='cuda:0', grad_fn=<MulBackward0>)}
[234:56399]  loss_total_vae=9782.588  loss_recon=6655.830  loss_kld=858.646  loss_prediction=29.068  loss_true_values=2239.043  loss_total_vae_epoch=9724.317  
tracking changes
Validation stop evaluation
56399 234
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
205  206.0  6665.540396  857.809045  0.359801  1.634594  0.843958
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723

[235 rows x 6 columns]
Now val counter at: 1
Latent 0.3597358564535777   0.36092694103717804
BCE 1.877392546335856 1.8171293258666992
Saved model at epoch 235
Losses: {'total_vae': tensor(9602.6006, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6507.8750, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(900.3552, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(34.1805, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2160.1902, device='cuda:0', grad_fn=<MulBackward0>)}
[235:56639]  loss_total_vae=9651.936  loss_recon=6523.541  loss_kld=876.663  loss_prediction=23.829  loss_true_values=2227.902  loss_total_vae_epoch=9725.171  
tracking changes
Validation stop evaluation
56639 235
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
206  207.0  6664.493729  873.813670  0.358973  1.689095  0.841481
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859

[236 rows x 6 columns]
Now val counter at: 1
Latent 0.3605449855327606   0.36010513305664055
BCE 1.856010623772939 1.8222273720635307
Losses: {'total_vae': tensor(9663.1367, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6594.0962, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(892.4387, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(29.3481, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2147.2537, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
56879 236
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
207  208.0  6666.112521  847.587345  0.362012  1.692331  0.839584
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592

[237 rows x 6 columns]
Now val counter at: 2
Latent 0.3607343902190526   0.3595427602529526
BCE 1.8425911347071329 1.842957990699344
Saved model at epoch 237
Losses: {'total_vae': tensor(9805.9229, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6711.2266, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(855.9284, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(37.8237, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2200.9443, device='cuda:0', grad_fn=<MulBackward0>)}
[237:57119]  loss_total_vae=9856.563  loss_recon=6739.055  loss_kld=867.874  loss_prediction=35.673  loss_true_values=2213.962  loss_total_vae_epoch=9721.162  
tracking changes
Validation stop evaluation
57119 237
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
208  209.0  6661.254688  859.184507  0.360649  1.697193  0.840787
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823

[238 rows x 6 columns]
Now val counter at: 1
Latent 0.35977189739545185   0.3596533957454893
BCE 1.8953331391016641 1.8567178554005093
Losses: {'total_vae': tensor(9689.0039, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6638.4292, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(851.7632, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(28.4117, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2170.3994, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
57359 238
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
209  210.0  6672.057625  850.925517  0.360903  1.672688  0.842373
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807

[239 rows x 6 columns]
Now val counter at: 2
Latent 0.36086961527665457   0.36033841073513023
BCE 1.889232087135315 1.858664768271976
Losses: {'total_vae': tensor(9705.3350, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6562.0625, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(856.0004, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(64.9939, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2222.2781, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
57599 239
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
210  211.0  6668.218354  844.636039  0.362140  1.668963  0.840894
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949

[240 rows x 6 columns]
Latent 0.3582361767689387   0.36035042438242165
BCE 1.849642566839854 1.8646449658605786
Saved model at epoch 240
Losses: {'total_vae': tensor(9717.8320, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6603.1548, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(874.8949, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(31.3138, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2208.4678, device='cuda:0', grad_fn=<MulBackward0>)}
[240:57839]  loss_total_vae=9666.611  loss_recon=6545.879  loss_kld=856.714  loss_prediction=35.753  loss_true_values=2228.266  loss_total_vae_epoch=9713.512  
tracking changes
Validation stop evaluation
57839 240
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
211  212.0  6668.980313  843.502110  0.362035  1.688913  0.840994
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900

[241 rows x 6 columns]
Now val counter at: 1
Latent 0.35934014320373536   0.36045863429705305
BCE 1.8711943984031678 1.8757187869813707
Saved model at epoch 241
Losses: {'total_vae': tensor(9724.8789, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6593.8843, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(870.4153, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(35.9013, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2224.6772, device='cuda:0', grad_fn=<MulBackward0>)}
[241:58079]  loss_total_vae=9665.939  loss_recon=6578.337  loss_kld=868.225  loss_prediction=31.463  loss_true_values=2187.913  loss_total_vae_epoch=9718.017  
tracking changes
Validation stop evaluation
58079 241
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
212  213.0  6667.641979  848.587722  0.362590  1.724222  0.842561
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240

[242 rows x 6 columns]
Now val counter at: 1
Latent 0.36036403278509777   0.3596258964803483
BCE 1.8565225323041281 1.8780692643589443
Saved model at epoch 242
Losses: {'total_vae': tensor(9736.1387, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6665.5474, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(857.4497, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(27.9200, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2185.2212, device='cuda:0', grad_fn=<MulBackward0>)}
[242:58319]  loss_total_vae=9707.068  loss_recon=6623.212  loss_kld=868.720  loss_prediction=54.139  loss_true_values=2160.997  loss_total_vae_epoch=9709.199  
tracking changes
Validation stop evaluation
58319 242
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
213  214.0  6665.568104  864.941418  0.359398  1.712308  0.845938
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689

[243 rows x 6 columns]
Now val counter at: 1
Latent 0.36182662347952527   0.35948197841644286
BCE 1.9042272011439005 1.8700230174594459
Losses: {'total_vae': tensor(9745.8867, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6621.8374, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(858.1066, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(47.8916, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2218.0515, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
58559 243
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
214  215.0  6667.385625  851.707294  0.360679  1.726804  0.840057
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251

[244 rows x 6 columns]
Now val counter at: 2
Latent 0.362946222225825   0.35931345091925726
BCE 1.88868305683136 1.8591198325157166
Losses: {'total_vae': tensor(9703.7480, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6624.7886, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(871.1313, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(20.7756, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2187.0525, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
58799 244
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
215  216.0  6667.795688  881.676390  0.358933  1.751190  0.843503
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858

[245 rows x 6 columns]
Now val counter at: 3
Latent 0.36164257129033406   0.36051026648945284
BCE 1.9675819834073385 1.8773147106170656
Losses: {'total_vae': tensor(9748.7793, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6631.0762, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(875.0235, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(58.0136, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2184.6663, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
59039 245
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
216  217.0  6664.786167  843.704325  0.361274  1.744950  0.841845
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620

[246 rows x 6 columns]
Latent 0.3579699198404948   0.36171229283014933
BCE 1.8828378995259603 1.8831442634264628
Saved model at epoch 246
Losses: {'total_vae': tensor(9695.0234, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6656.6323, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(878.1672, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(23.2425, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2136.9807, device='cuda:0', grad_fn=<MulBackward0>)}
[246:59279]  loss_total_vae=9673.787  loss_recon=6676.889  loss_kld=846.369  loss_prediction=30.584  loss_true_values=2119.945  loss_total_vae_epoch=9702.951  
tracking changes
Validation stop evaluation
59279 246
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
217  218.0  6665.218063  851.925773  0.361954  1.735474  0.841948
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686

[247 rows x 6 columns]
Now val counter at: 1
Latent 0.3615855276584625   0.3621384723318948
BCE 1.9295497258504233 1.9201640804608664
Saved model at epoch 247
Losses: {'total_vae': tensor(9540.5342, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6460.6118, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(873.2302, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(33.0114, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2173.6814, device='cuda:0', grad_fn=<MulBackward0>)}
[247:59519]  loss_total_vae=9751.636  loss_recon=6679.626  loss_kld=873.646  loss_prediction=43.602  loss_true_values=2154.762  loss_total_vae_epoch=9699.131  
tracking changes
Validation stop evaluation
59519 247
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
218  219.0  6664.522437  855.913053  0.360153  1.744019  0.840496
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772

[248 rows x 6 columns]
Saved model at epoch 248
Losses: {'total_vae': tensor(9779.2949, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6710.9165, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(854.7500, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(19.6965, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2193.9319, device='cuda:0', grad_fn=<MulBackward0>)}
[248:59759]  loss_total_vae=9649.947  loss_recon=6593.660  loss_kld=885.245  loss_prediction=29.078  loss_true_values=2141.966  loss_total_vae_epoch=9693.638  
tracking changes
Validation stop evaluation
59759 248
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
219  220.0  6664.935750  875.414909  0.357216  1.725792  0.841248
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375

[249 rows x 6 columns]
Saved model at epoch 249
Losses: {'total_vae': tensor(9654.9414, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6553.7822, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(862.6716, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(33.2925, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2205.1953, device='cuda:0', grad_fn=<MulBackward0>)}
[249:59999]  loss_total_vae=9646.055  loss_recon=6510.208  loss_kld=861.766  loss_prediction=23.613  loss_true_values=2250.467  loss_total_vae_epoch=9699.840  
tracking changes
Validation stop evaluation
59999 249
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
220  221.0  6668.443396  859.613198  0.359589  1.743309  0.842209
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764

[250 rows x 6 columns]
Now val counter at: 1
Latent 0.3596712907155355   0.36005661884943646
BCE 1.9557242314020793 1.9015867895550198
Saved model at epoch 250
Losses: {'total_vae': tensor(9650.9922, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6547.1431, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(887.8156, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(29.4630, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2186.5706, device='cuda:0', grad_fn=<MulBackward0>)}
[250:60239]  loss_total_vae=9680.096  loss_recon=6590.811  loss_kld=867.014  loss_prediction=30.217  loss_true_values=2192.053  loss_total_vae_epoch=9710.911  
tracking changes
Validation stop evaluation
60239 250
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
221  222.0  6664.116667  864.214508  0.359640  1.781334  0.839520
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542

[251 rows x 6 columns]
Now val counter at: 1
Latent 0.36122872730096184   0.3608542912536197
BCE 1.9548176050186157 1.9045685595936244
Losses: {'total_vae': tensor(9772.7256, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6648.7988, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(866.9214, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(29.4995, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2227.5061, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
60479 251
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
222  223.0  6668.817250  832.683706  0.363178  1.773643  0.842711
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550

[252 rows x 6 columns]
Latent 0.3584402491648992   0.36021621227264405
BCE 1.9085164984067282 1.9132933947775097
Saved model at epoch 252
Losses: {'total_vae': tensor(9771.7031, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6778.3325, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(887.6031, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(14.7770, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2090.9905, device='cuda:0', grad_fn=<MulBackward0>)}
[252:60719]  loss_total_vae=9580.256  loss_recon=6476.840  loss_kld=875.125  loss_prediction=31.398  loss_true_values=2196.894  loss_total_vae_epoch=9689.219  
tracking changes
Validation stop evaluation
60719 252
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
223  224.0  6667.270083  837.968644  0.363130  1.828498  0.842328
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949

[253 rows x 6 columns]
Now val counter at: 1
Latent 0.3617009301980337   0.3604209850231806
BCE 1.961789619922638 1.93410834868749
Losses: {'total_vae': tensor(9558.3057, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6426.6113, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(847.5967, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(22.9057, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2261.1917, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
60959 253
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
224  225.0  6663.127312  876.592881  0.358326  1.778427  0.842157
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593

[254 rows x 6 columns]
Latent 0.36165329813957214   0.3597800890604655
BCE 1.9370843291282653 1.9396861116091408
Saved model at epoch 254
Losses: {'total_vae': tensor(9671.3262, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6606.3350, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(834.8768, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(12.9887, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2217.1260, device='cuda:0', grad_fn=<MulBackward0>)}
[254:61199]  loss_total_vae=9674.667  loss_recon=6696.954  loss_kld=849.333  loss_prediction=22.356  loss_true_values=2106.025  loss_total_vae_epoch=9698.880  
tracking changes
Validation stop evaluation
61199 254
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
225  226.0  6669.306417  849.076595  0.361326  1.798460  0.841758
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875

[255 rows x 6 columns]
Now val counter at: 1
Latent 0.35889517068862914   0.36045663555463153
BCE 1.9394967118899027 1.9417079077826607
Saved model at epoch 255
Losses: {'total_vae': tensor(9866.9639, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6798.5522, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(876.0532, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(21.3602, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2170.9978, device='cuda:0', grad_fn=<MulBackward0>)}
[255:61439]  loss_total_vae=9605.553  loss_recon=6575.715  loss_kld=852.115  loss_prediction=17.392  loss_true_values=2160.331  loss_total_vae_epoch=9685.402  
tracking changes
Validation stop evaluation
61439 255
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
226  227.0  6659.239188  873.019822  0.359933  1.844417  0.842657
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413

[256 rows x 6 columns]
Now val counter at: 1
Latent 0.3600771963596344   0.36059815916750165
BCE 1.9481425285339355 1.9357968158192105
Saved model at epoch 256
Losses: {'total_vae': tensor(9723.4131, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6721.0024, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(875.0005, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(28.6825, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2098.7275, device='cuda:0', grad_fn=<MulBackward0>)}
[256:61679]  loss_total_vae=9579.777  loss_recon=6523.197  loss_kld=878.093  loss_prediction=23.994  loss_true_values=2154.493  loss_total_vae_epoch=9680.007  
tracking changes
Validation stop evaluation
61679 256
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
227  228.0  6666.915000  854.175330  0.360866  1.808055  0.841974
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169

[257 rows x 6 columns]
Now val counter at: 1
Latent 0.35857709546883904   0.36074979967541165
BCE 1.9800775051116943 1.9461235536469352
Saved model at epoch 257
Losses: {'total_vae': tensor(9545.2500, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6495.2720, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(883.5566, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(38.5836, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2127.8381, device='cuda:0', grad_fn=<MulBackward0>)}
[257:61919]  loss_total_vae=9733.538  loss_recon=6615.333  loss_kld=860.077  loss_prediction=20.353  loss_true_values=2237.776  loss_total_vae_epoch=9689.165  
tracking changes
Validation stop evaluation
61919 257
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
228  229.0  6667.716083  845.774241  0.362036  1.827379  0.841918
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021

[258 rows x 6 columns]
Now val counter at: 1
Latent 0.3610453397035599   0.36020855506261196
BCE 1.9585472067197165 1.9415745231840347
Losses: {'total_vae': tensor(9621.5469, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6551.9893, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(853.5080, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(19.8429, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2196.2068, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
62159 258
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
229  230.0  6668.356125  860.830212  0.359762  1.810428  0.844036
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490

[259 rows x 6 columns]
Now val counter at: 2
Latent 0.36018153627713523   0.3591831541723675
BCE 1.9863805532455445 1.9559055818451776
Losses: {'total_vae': tensor(9558.3848, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6481.6323, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(848.9133, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(27.3060, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2200.5327, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
62399 259
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
230  231.0  6670.607583  855.979867  0.361145  1.821456  0.846113
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605

[260 rows x 6 columns]
Now val counter at: 3
Latent 0.36350495417912804   0.3598998771773445
BCE 2.0042591373125713 1.9622557467884487
Losses: {'total_vae': tensor(9553.2422, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6476.5811, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(870.9757, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(33.2390, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2172.4470, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
62639 260
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
231  232.0  6668.107000  846.413656  0.361423  1.815201  0.841432
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214

[261 rows x 6 columns]
Latent 0.36165067354838054   0.35993465714984474
BCE 1.9663586139678955 1.9750017550256516
Saved model at epoch 261
Losses: {'total_vae': tensor(9638.6895, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6583.9980, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(862.6481, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(23.1130, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2168.9302, device='cuda:0', grad_fn=<MulBackward0>)}
[261:62879]  loss_total_vae=9697.641  loss_recon=6590.071  loss_kld=872.588  loss_prediction=75.509  loss_true_values=2159.473  loss_total_vae_epoch=9676.005  
tracking changes
Validation stop evaluation
62879 261
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
232  233.0  6667.625229  863.734314  0.360213  1.814731  0.842715
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001

[262 rows x 6 columns]
Now val counter at: 1
Latent 0.3576124449570974   0.36157727671994105
BCE 1.9772181312243144 1.9830622990926108
Saved model at epoch 262
Losses: {'total_vae': tensor(9632.7422, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6530.5010, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(885.1993, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(35.1625, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2181.8792, device='cuda:0', grad_fn=<MulBackward0>)}
[262:63119]  loss_total_vae=9550.654  loss_recon=6500.716  loss_kld=854.934  loss_prediction=31.599  loss_true_values=2163.405  loss_total_vae_epoch=9676.813  
tracking changes
Validation stop evaluation
63119 262
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
233  234.0  6666.611146  871.373075  0.358679  1.836750  0.843291
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783

[263 rows x 6 columns]
Now val counter at: 1
Latent 0.3599467744429906   0.36177905466821453
BCE 2.01424503326416 1.9856661015086703
Saved model at epoch 263
Losses: {'total_vae': tensor(9598.8535, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6484.9785, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(860.6440, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(69.3535, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2183.8770, device='cuda:0', grad_fn=<MulBackward0>)}
[263:63359]  loss_total_vae=9682.359  loss_recon=6624.019  loss_kld=906.116  loss_prediction=41.094  loss_true_values=2111.130  loss_total_vae_epoch=9683.169  
tracking changes
Validation stop evaluation
63359 263
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
234  235.0  6663.384729  875.988377  0.359736  1.877393  0.839723
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613

[264 rows x 6 columns]
Now val counter at: 1
Latent 0.3613533784945806   0.36092269089486867
BCE 2.042046256860097 1.9826119608349273
Losses: {'total_vae': tensor(9753.7715, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6696.4624, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(861.5315, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(37.9788, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2157.7986, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
63599 264
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
235  236.0  6665.844687  869.156073  0.360545  1.856011  0.844859
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823

[265 rows x 6 columns]
Latent 0.35769058962663014   0.3597366309828229
BCE 1.999422307809194 1.98594059281879
Saved model at epoch 265
Losses: {'total_vae': tensor(9775.0596, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6636.2617, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(876.1409, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(41.1074, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2221.5500, device='cuda:0', grad_fn=<MulBackward0>)}
[265:63839]  loss_total_vae=9506.074  loss_recon=6547.601  loss_kld=841.376  loss_prediction=27.299  loss_true_values=2089.799  loss_total_vae_epoch=9666.900  
tracking changes
Validation stop evaluation
63839 265
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
236  237.0  6666.558062  857.645502  0.360734  1.842591  0.840592
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289

[266 rows x 6 columns]
Now val counter at: 1
Latent 0.361848517258962   0.3596375326315562
BCE 2.03385591506958 2.011169807116191
Losses: {'total_vae': tensor(9561.6943, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6544.0874, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(874.1218, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(28.7145, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2114.7712, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
64079 266
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
237  238.0  6667.693292  869.075893  0.359772  1.895333  0.841823
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279

[267 rows x 6 columns]
Now val counter at: 2
Latent 0.362493089834849   0.35966358085473377
BCE 1.9887339313824972 2.0185711993111504
Saved model at epoch 267
Losses: {'total_vae': tensor(9665.8262, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6675.0913, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(818.2610, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(18.1579, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2154.3159, device='cuda:0', grad_fn=<MulBackward0>)}
[267:64319]  loss_total_vae=9871.613  loss_recon=6769.332  loss_kld=847.793  loss_prediction=28.350  loss_true_values=2226.139  loss_total_vae_epoch=9668.294  
tracking changes
Validation stop evaluation
64319 267
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
238  239.0  6669.079625  865.321442  0.360870  1.889232  0.841807
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298

[268 rows x 6 columns]
Now val counter at: 1
Latent 0.3608380953470866   0.3602974951267242
BCE 2.025704526901245 2.025108159912957
Losses: {'total_vae': tensor(9557.6396, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6513.4863, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(885.2207, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(13.5391, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2145.3933, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
64559 268
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
239  240.0  6665.118458  879.633205  0.358236  1.849643  0.845949
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640

[269 rows x 6 columns]
Now val counter at: 2
Latent 0.36140491167704264   0.3606773989068137
BCE 2.000586255391439 2.007337384753757
Saved model at epoch 269
Losses: {'total_vae': tensor(9659.4570, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6645.4019, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(857.6797, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(38.4131, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2117.9624, device='cuda:0', grad_fn=<MulBackward0>)}
[269:64799]  loss_total_vae=9658.090  loss_recon=6657.702  loss_kld=851.084  loss_prediction=31.609  loss_true_values=2117.695  loss_total_vae_epoch=9665.934  
tracking changes
Validation stop evaluation
64799 269
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
240  241.0  6662.437750  870.480048  0.359340  1.871194  0.842900
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379

[270 rows x 6 columns]
Now val counter at: 1
Latent 0.36411118110020957   0.3617265674802992
BCE 2.06417692899704 2.016098124451107
Losses: {'total_vae': tensor(9693.8057, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6668.2080, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(849.0996, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(39.3657, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2137.1323, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
65039 270
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
241  242.0  6669.050000  854.094942  0.360364  1.856523  0.845240
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990

[271 rows x 6 columns]
Latent 0.36122753421465553   0.36157869895299277
BCE 2.0379350980122886 2.005008237891727
Saved model at epoch 271
Losses: {'total_vae': tensor(9701.3184, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6634.0049, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(860.2190, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(23.0472, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2184.0469, device='cuda:0', grad_fn=<MulBackward0>)}
[271:65279]  loss_total_vae=9578.477  loss_recon=6487.616  loss_kld=872.480  loss_prediction=26.746  loss_true_values=2191.634  loss_total_vae_epoch=9667.422  
tracking changes
Validation stop evaluation
65279 271
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
242  243.0  6666.721375  855.708811  0.361827  1.904227  0.843689
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787

[272 rows x 6 columns]
Saved model at epoch 272
Losses: {'total_vae': tensor(9531.6143, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6504.9517, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(859.6650, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(16.1546, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2150.8428, device='cuda:0', grad_fn=<MulBackward0>)}
[272:65519]  loss_total_vae=9728.188  loss_recon=6649.364  loss_kld=834.631  loss_prediction=75.626  loss_true_values=2168.567  loss_total_vae_epoch=9657.763  
tracking changes
Validation stop evaluation
65519 272
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
243  244.0  6670.236479  841.497146  0.362946  1.888683  0.841251
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888

[273 rows x 6 columns]
Now val counter at: 1
Latent 0.36334180732568105   0.36224787566396927
BCE 2.119424060980479 2.034232760800256
Losses: {'total_vae': tensor(9685.0322, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6635.4829, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(840.8160, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(59.6676, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2149.0659, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
65759 273
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
244  245.0  6668.252042  865.609393  0.361643  1.967582  0.841858
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088

[274 rows x 6 columns]
Latent 0.36122864882151284   0.36167814003096693
BCE 2.022790284951528 2.045347065395779
Saved model at epoch 274
Losses: {'total_vae': tensor(9717.6650, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6693.6152, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(879.5293, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(23.6424, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2120.8779, device='cuda:0', grad_fn=<MulBackward0>)}
[274:65999]  loss_total_vae=9627.963  loss_recon=6624.181  loss_kld=872.192  loss_prediction=19.949  loss_true_values=2111.642  loss_total_vae_epoch=9657.340  
tracking changes
Validation stop evaluation
65999 274
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
245  246.0  6666.683146  885.422825  0.357970  1.882838  0.844620
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769

[275 rows x 6 columns]
Now val counter at: 1
Latent 0.36051491300264993   0.361421682106124
BCE 2.0869243144989014 2.0637627760569255
Saved model at epoch 275
Losses: {'total_vae': tensor(9620.7197, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6598.0898, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(867.0583, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(27.3145, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2128.2568, device='cuda:0', grad_fn=<MulBackward0>)}
[275:66239]  loss_total_vae=9659.492  loss_recon=6604.169  loss_kld=883.915  loss_prediction=31.784  loss_true_values=2139.624  loss_total_vae_epoch=9656.556  
tracking changes
Validation stop evaluation
66239 275
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
246  247.0  6668.964042  860.768058  0.361586  1.929550  0.845686
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216

[276 rows x 6 columns]
Now val counter at: 1
Latent 0.36065098146597546   0.3614220536417431
BCE 2.067852091789246 2.0587145050366717
Saved model at epoch 276
Losses: {'total_vae': tensor(9483.1396, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6503.8047, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(839.3767, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(22.9059, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2117.0520, device='cuda:0', grad_fn=<MulBackward0>)}
[276:66479]  loss_total_vae=9593.551  loss_recon=6528.309  loss_kld=876.893  loss_prediction=27.114  loss_true_values=2161.235  loss_total_vae_epoch=9663.160  
tracking changes
Validation stop evaluation
66479 276
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
247  248.0  6667.198187  860.115611  0.360614  1.892373  0.847772
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877

[277 rows x 6 columns]
Now val counter at: 1
Latent 0.3625698616107305   0.36169512304994794
BCE 2.0801104346911115 2.0763795534769693
Losses: {'total_vae': tensor(9576.6885, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6571.7329, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(852.6059, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(43.7919, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2108.5576, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
66719 277
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
248  249.0  6666.788542  860.066278  0.360363  1.891783  0.841375
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123

[278 rows x 6 columns]
Latent 0.3598150153954824   0.36079818109671274
BCE 2.034339157740275 2.0591888970798915
Saved model at epoch 278
Losses: {'total_vae': tensor(9654.5146, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6632.7456, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(902.1733, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(15.7638, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2103.8323, device='cuda:0', grad_fn=<MulBackward0>)}
[278:66959]  loss_total_vae=9679.620  loss_recon=6628.554  loss_kld=848.748  loss_prediction=16.928  loss_true_values=2185.390  loss_total_vae_epoch=9651.789  
tracking changes
Validation stop evaluation
66959 278
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
249  250.0  6668.693854  870.296529  0.359671  1.955724  0.844764
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963

[279 rows x 6 columns]
Now val counter at: 1
Latent 0.3645630111296972   0.36124525202645197
BCE 2.0816925366719565 2.078295613659753
Losses: {'total_vae': tensor(9624.2490, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6591.2612, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(834.9371, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(16.8902, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2181.1606, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
67199 279
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
250  251.0  6668.902000  853.789223  0.361229  1.954818  0.841542
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787

[280 rows x 6 columns]
Latent 0.3603899389505386   0.3610119528240628
BCE 2.052495769659678 2.0607672280735443
Saved model at epoch 280
Losses: {'total_vae': tensor(9431.7051, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6471.7441, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(895.7211, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(15.3012, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2048.9382, device='cuda:0', grad_fn=<MulBackward0>)}
[280:67439]  loss_total_vae=9767.255  loss_recon=6678.210  loss_kld=885.978  loss_prediction=52.796  loss_true_values=2150.270  loss_total_vae_epoch=9648.347  
tracking changes
Validation stop evaluation
67439 280
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
251  252.0  6665.091917  879.830373  0.358440  1.908516  0.843550
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829

[281 rows x 6 columns]
Now val counter at: 1
Latent 0.3604483544826508   0.36231596271197003
BCE 2.1180200219154357 2.0653807097011145
Saved model at epoch 281
Losses: {'total_vae': tensor(9701.6113, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6630.7974, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(897.7365, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(38.0146, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2135.0630, device='cuda:0', grad_fn=<MulBackward0>)}
[281:67679]  loss_total_vae=9656.098  loss_recon=6639.650  loss_kld=859.398  loss_prediction=26.887  loss_true_values=2130.163  loss_total_vae_epoch=9660.582  
tracking changes
Validation stop evaluation
67679 281
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
252  253.0  6670.759167  847.441744  0.361701  1.961790  0.836949
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638

[282 rows x 6 columns]
Now val counter at: 1
Latent 0.3605486532052358   0.36158932182523945
BCE 2.05704079469045 2.0561758213573036
Saved model at epoch 282
Losses: {'total_vae': tensor(9532.8770, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6536.9932, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(877.4512, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(20.5401, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2097.8921, device='cuda:0', grad_fn=<MulBackward0>)}
[282:67919]  loss_total_vae=9701.562  loss_recon=6655.900  loss_kld=872.401  loss_prediction=14.349  loss_true_values=2158.911  loss_total_vae_epoch=9644.759  
tracking changes
Validation stop evaluation
67919 282
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
253  254.0  6666.860875  845.012726  0.361653  1.937084  0.842593
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512

[283 rows x 6 columns]
Now val counter at: 1
Latent 0.35976371467113494   0.3618004348542955
BCE 2.0733986377716063 2.084069442749023
Saved model at epoch 283
Losses: {'total_vae': tensor(9752.4141, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6692.3838, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(857.6309, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(10.9028, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2191.4966, device='cuda:0', grad_fn=<MulBackward0>)}
[283:68159]  loss_total_vae=9680.123  loss_recon=6629.843  loss_kld=840.333  loss_prediction=19.442  loss_true_values=2190.505  loss_total_vae_epoch=9645.184  
tracking changes
Validation stop evaluation
68159 283
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
254  255.0  6664.458750  873.662748  0.358895  1.939497  0.843875
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610

[284 rows x 6 columns]
Now val counter at: 1
Latent 0.3621986428896586   0.3604623155461417
BCE 2.098156801859538 2.0758521954218545
Losses: {'total_vae': tensor(9704.3105, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6660.3828, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(860.1084, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(24.2101, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2159.6091, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
68399 284
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
255  256.0  6676.185250  867.958213  0.360077  1.948143  0.843413
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591

[285 rows x 6 columns]
Now val counter at: 2
Latent 0.3596200595299403   0.3602535741196739
BCE 2.102594784895579 2.0828198181258304
Saved model at epoch 285
Losses: {'total_vae': tensor(9534.8408, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6511.0249, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(874.6193, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(19.5660, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2129.6311, device='cuda:0', grad_fn=<MulBackward0>)}
[285:68639]  loss_total_vae=9612.079  loss_recon=6606.230  loss_kld=874.664  loss_prediction=27.422  loss_true_values=2103.763  loss_total_vae_epoch=9642.247  
tracking changes
Validation stop evaluation
68639 285
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
256  257.0  6664.874833  878.602844  0.358577  1.980078  0.843169
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149

[286 rows x 6 columns]
Now val counter at: 1
Latent 0.36318293114503225   0.3608370035886765
BCE 2.113913627465566 2.076198744773865
Losses: {'total_vae': tensor(9565.5244, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6563.1211, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(841.5994, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(19.5178, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2141.2864, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
68879 286
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
257  258.0  6667.402229  858.824034  0.361045  1.958547  0.842021
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941

[287 rows x 6 columns]
Now val counter at: 2
Latent 0.3636226465304693   0.3605274723635779
BCE 2.126632805665334 2.091383408175574
Losses: {'total_vae': tensor(9813.8281, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6745.7622, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(844.6451, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(17.8982, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2205.5225, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
69119 287
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
258  259.0  6670.490000  862.794775  0.360182  1.986381  0.845490
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186

[288 rows x 6 columns]
Now val counter at: 3
Latent 0.36161582668622333   0.3616672111882104
BCE 2.137724490960439 2.1048884047402274
Saved model at epoch 288
Losses: {'total_vae': tensor(9753.8506, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6628.8057, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(838.2124, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(14.3128, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2272.5193, device='cuda:0', grad_fn=<MulBackward0>)}
[288:69359]  loss_total_vae=9751.536  loss_recon=6692.545  loss_kld=863.840  loss_prediction=30.215  loss_true_values=2164.936  loss_total_vae_epoch=9639.881  
tracking changes
Validation stop evaluation
69359 288
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
259  260.0  6673.094646  841.488151  0.363505  2.004259  0.841605
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725

[289 rows x 6 columns]
Now val counter at: 1
Latent 0.3567158381144206   0.36214187906848067
BCE 2.1516512632369995 2.1143804060088267
Saved model at epoch 289
Losses: {'total_vae': tensor(9736.8008, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6697.6431, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(908.7226, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(45.7188, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2084.7158, device='cuda:0', grad_fn=<MulBackward0>)}
[289:69599]  loss_total_vae=9523.229  loss_recon=6460.241  loss_kld=882.391  loss_prediction=29.851  loss_true_values=2150.747  loss_total_vae_epoch=9636.318  
tracking changes
Validation stop evaluation
69599 289
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
260  261.0  6668.314521  855.867753  0.361651  1.966359  0.845214
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795

[290 rows x 6 columns]
Now val counter at: 1
Latent 0.36375756164391837   0.36280713478724164
BCE 2.117755071322123 2.1260903080304465
Saved model at epoch 290
Losses: {'total_vae': tensor(9412.9277, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6444.3154, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(858.8168, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(18.1969, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2091.5991, device='cuda:0', grad_fn=<MulBackward0>)}
[290:69839]  loss_total_vae=9615.164  loss_recon=6567.563  loss_kld=855.652  loss_prediction=71.188  loss_true_values=2120.760  loss_total_vae_epoch=9635.915  
tracking changes
Validation stop evaluation
69839 290
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
261  262.0  6672.603667  882.848000  0.357612  1.977218  0.845001
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368

[291 rows x 6 columns]
Now val counter at: 1
Latent 0.3633938699960709   0.36065143711037106
BCE 2.174458905061086 2.1386695199542576
Losses: {'total_vae': tensor(9683.6689, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6654.7549, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(868.0516, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(33.2933, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2127.5686, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
70079 291
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
262  263.0  6670.888354  869.423739  0.359947  2.014245  0.844783
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933

[292 rows x 6 columns]
Latent 0.36162727971871694   0.3606964088148541
BCE 2.1344834208488463 2.1357102751731873
Saved model at epoch 292
Losses: {'total_vae': tensor(9596.5557, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6533.8960, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(865.2560, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(25.0198, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2172.3835, device='cuda:0', grad_fn=<MulBackward0>)}
[292:70319]  loss_total_vae=9639.596  loss_recon=6577.269  loss_kld=882.394  loss_prediction=33.692  loss_true_values=2146.240  loss_total_vae_epoch=9636.166  
tracking changes
Validation stop evaluation
70319 292
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
263  264.0  6683.059354  857.016150  0.361353  2.042046  0.843613
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933
292  293.0  6673.129750  871.572215  0.359750  2.114055  0.845317

[293 rows x 6 columns]
Saved model at epoch 293
Losses: {'total_vae': tensor(9477.3066, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6486.0469, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(857.9219, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(11.6751, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2121.6621, device='cuda:0', grad_fn=<MulBackward0>)}
[293:70559]  loss_total_vae=9482.456  loss_recon=6487.297  loss_kld=886.110  loss_prediction=26.359  loss_true_values=2082.690  loss_total_vae_epoch=9632.315  
tracking changes
Validation stop evaluation
70559 293
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
264  265.0  6662.911667  888.001980  0.357691  1.999422  0.842823
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933
292  293.0  6673.129750  871.572215  0.359750  2.114055  0.845317
293  294.0  6684.176438  877.790686  0.360417  2.145494  0.844547

[294 rows x 6 columns]
Now val counter at: 1
Latent 0.3604168305794398   0.36292623711956873
BCE 2.1454938809076944 2.142232465744019
Saved model at epoch 294
Losses: {'total_vae': tensor(9580.4004, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6527.1787, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(879.7310, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(39.4653, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2134.0254, device='cuda:0', grad_fn=<MulBackward0>)}
[294:70799]  loss_total_vae=9649.225  loss_recon=6656.421  loss_kld=871.230  loss_prediction=14.535  loss_true_values=2107.038  loss_total_vae_epoch=9633.727  
tracking changes
Validation stop evaluation
70799 294
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
265  266.0  6676.235250  849.130214  0.361849  2.033856  0.843289
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933
292  293.0  6673.129750  871.572215  0.359750  2.114055  0.845317
293  294.0  6684.176438  877.790686  0.360417  2.145494  0.844547
294  295.0  6669.354250  882.878518  0.358607  2.165440  0.837530

[295 rows x 6 columns]
Now val counter at: 1
Latent 0.3586066176493963   0.36159024039904275
BCE 2.1654404083887737 2.1409992191526626
Saved model at epoch 295
Losses: {'total_vae': tensor(9545.4727, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6578.8457, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(901.9584, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(28.1639, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2036.5040, device='cuda:0', grad_fn=<MulBackward0>)}
[295:71039]  loss_total_vae=9702.793  loss_recon=6647.624  loss_kld=886.621  loss_prediction=15.198  loss_true_values=2153.352  loss_total_vae_epoch=9657.740  
tracking changes
Validation stop evaluation
71039 295
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
266  267.0  6674.444042  840.933331  0.362493  1.988734  0.842279
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933
292  293.0  6673.129750  871.572215  0.359750  2.114055  0.845317
293  294.0  6684.176438  877.790686  0.360417  2.145494  0.844547
294  295.0  6669.354250  882.878518  0.358607  2.165440  0.837530
295  296.0  6673.586646  858.408370  0.362924  2.160825  0.845195

[296 rows x 6 columns]
Now val counter at: 1
Latent 0.36292407612005867   0.3605978939268324
BCE 2.160825447241465 2.1313442111015317
Losses: {'total_vae': tensor(9830.7617, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6816.5601, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(854.8894, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(24.8227, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2134.4902, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
71279 296
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
267  268.0  6677.688396  861.933720  0.360838  2.025705  0.840298
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933
292  293.0  6673.129750  871.572215  0.359750  2.114055  0.845317
293  294.0  6684.176438  877.790686  0.360417  2.145494  0.844547
294  295.0  6669.354250  882.878518  0.358607  2.165440  0.837530
295  296.0  6673.586646  858.408370  0.362924  2.160825  0.845195
296  297.0  6675.506604  867.952435  0.360609  2.137251  0.844361

[297 rows x 6 columns]
Latent 0.3606092244386673   0.3595910065703922
BCE 2.1372507214546204 2.141663206948174
Saved model at epoch 297
Losses: {'total_vae': tensor(9610.1074, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6611.0498, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(861.7766, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(21.6939, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2115.5874, device='cuda:0', grad_fn=<MulBackward0>)}
[297:71519]  loss_total_vae=9582.198  loss_recon=6534.093  loss_kld=852.804  loss_prediction=16.496  loss_true_values=2178.805  loss_total_vae_epoch=9617.899  
tracking changes
Validation stop evaluation
71519 297
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
268  269.0  6672.588562  853.182404  0.361405  2.000586  0.844640
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933
292  293.0  6673.129750  871.572215  0.359750  2.114055  0.845317
293  294.0  6684.176438  877.790686  0.360417  2.145494  0.844547
294  295.0  6669.354250  882.878518  0.358607  2.165440  0.837530
295  296.0  6673.586646  858.408370  0.362924  2.160825  0.845195
296  297.0  6675.506604  867.952435  0.360609  2.137251  0.844361
297  298.0  6676.618479  851.352633  0.362834  2.170461  0.842074

[298 rows x 6 columns]
Now val counter at: 1
Latent 0.36283396979173027   0.36064917478296493
BCE 2.1704608758290607 2.1572532455126443
Losses: {'total_vae': tensor(9536.4180, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6583.1538, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(875.6225, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(12.8254, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2064.8164, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
71759 298
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
269  270.0  6680.948875  848.273470  0.364111  2.064177  0.842379
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933
292  293.0  6673.129750  871.572215  0.359750  2.114055  0.845317
293  294.0  6684.176438  877.790686  0.360417  2.145494  0.844547
294  295.0  6669.354250  882.878518  0.358607  2.165440  0.837530
295  296.0  6673.586646  858.408370  0.362924  2.160825  0.845195
296  297.0  6675.506604  867.952435  0.360609  2.137251  0.844361
297  298.0  6676.618479  851.352633  0.362834  2.170461  0.842074
298  299.0  6665.216667  894.840609  0.358950  2.199059  0.842885

[299 rows x 6 columns]
Now val counter at: 2
Latent 0.3589499125878016   0.3607133060693741
BCE 2.199058683713277 2.1545055256949532
Saved model at epoch 299
Losses: {'total_vae': tensor(9701.7910, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6693.2178, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(884.3195, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(20.7674, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2103.4866, device='cuda:0', grad_fn=<MulBackward0>)}
[299:71999]  loss_total_vae=9591.160  loss_recon=6580.722  loss_kld=877.708  loss_prediction=29.248  loss_true_values=2103.482  loss_total_vae_epoch=9623.178  
tracking changes
Validation stop evaluation
71999 299
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
270  271.0  6674.023208  862.244765  0.361228  2.037935  0.843990
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933
292  293.0  6673.129750  871.572215  0.359750  2.114055  0.845317
293  294.0  6684.176438  877.790686  0.360417  2.145494  0.844547
294  295.0  6669.354250  882.878518  0.358607  2.165440  0.837530
295  296.0  6673.586646  858.408370  0.362924  2.160825  0.845195
296  297.0  6675.506604  867.952435  0.360609  2.137251  0.844361
297  298.0  6676.618479  851.352633  0.362834  2.170461  0.842074
298  299.0  6665.216667  894.840609  0.358950  2.199059  0.842885
299  300.0  6671.994396  847.069712  0.365517  2.169840  0.844708

[300 rows x 6 columns]
Now val counter at: 1
Latent 0.3655170679092407   0.36212242345015205
BCE 2.1698396881421407 2.1561790148417153
Losses: {'total_vae': tensor(9576.4658, device='cuda:0', grad_fn=<AddBackward0>), 'recon': tensor(6625.3184, device='cuda:0', grad_fn=<MulBackward0>), 'kld': tensor(849.8918, device='cuda:0', grad_fn=<MulBackward0>), 'prediction': tensor(12.5456, device='cuda:0', grad_fn=<MulBackward0>), 'true_values': tensor(2088.7104, device='cuda:0', grad_fn=<MulBackward0>)}
tracking changes
Validation stop evaluation
72239 300
     epoch          rec         kld    latent       bce       acc
0      1.0  7805.628583  403.991584  0.498629  1.575746  0.685440
1      2.0  7231.630021  549.471157  0.473170  1.516723  0.693317
2      3.0  7038.013646  601.302736  0.460612  1.462131  0.702794
3      4.0  6997.824708  617.590356  0.447967  1.389953  0.712503
4      5.0  6977.831833  615.358801  0.437847  1.275646  0.727423
5      6.0  6954.296458  668.642550  0.424504  1.189324  0.741439
6      7.0  6940.494938  703.286342  0.415612  1.117231  0.755217
7      8.0  6906.155125  728.463509  0.408462  1.068809  0.761118
8      9.0  6906.610708  692.562640  0.411568  1.052929  0.770244
9     10.0  6889.385917  699.185917  0.405700  1.018597  0.770950
10    11.0  6871.934062  703.327848  0.402478  0.992198  0.774184
11    12.0  6859.670375  703.651392  0.400425  0.982755  0.777777
12    13.0  6843.340250  765.582806  0.392923  0.967972  0.778994
13    14.0  6839.786146  730.520679  0.393815  0.943595  0.784945
14    15.0  6840.772167  757.279899  0.389707  0.936105  0.787053
15    16.0  6821.096208  733.216689  0.391825  0.931425  0.790168
16    17.0  6812.564292  756.751955  0.387787  0.918755  0.791657
17    18.0  6813.090542  715.670711  0.390980  0.918216  0.790858
18    19.0  6795.284854  775.642489  0.384447  0.898208  0.798756
19    20.0  6790.899667  751.678196  0.385111  0.903386  0.796429
20    21.0  6790.011167  744.133362  0.384410  0.889875  0.797227
21    22.0  6794.332604  704.479877  0.387746  0.893456  0.796055
22    23.0  6779.256750  763.521080  0.381121  0.883698  0.799609
23    24.0  6772.692062  742.665643  0.382641  0.884002  0.801009
24    25.0  6782.089292  726.713269  0.384948  0.886687  0.804571
25    26.0  6768.419125  730.938344  0.381371  0.868226  0.804484
26    27.0  6767.336687  763.777098  0.377407  0.869175  0.805329
27    28.0  6751.923958  796.319246  0.373156  0.863716  0.805323
28    29.0  6748.859646  756.941384  0.377829  0.856229  0.810357
29    30.0  6754.910792  748.573122  0.377851  0.849902  0.809258
..     ...          ...         ...       ...       ...       ...
271  272.0  6667.472938  879.653723  0.359696  2.033929  0.845787
272  273.0  6675.936292  858.159703  0.363342  2.119424  0.840888
273  274.0  6667.805688  864.424656  0.361229  2.022790  0.846088
274  275.0  6672.168062  866.995546  0.360515  2.086924  0.844769
275  276.0  6673.367958  864.577232  0.360651  2.067852  0.843216
276  277.0  6676.159979  853.517474  0.362570  2.080110  0.842877
277  278.0  6673.164208  869.152266  0.359815  2.034339  0.844123
278  279.0  6672.003083  838.480640  0.364563  2.081693  0.842963
279  280.0  6668.981229  869.838295  0.360390  2.052496  0.841787
280  281.0  6673.205854  874.832607  0.360448  2.118020  0.839829
281  282.0  6671.231396  869.338515  0.360549  2.057041  0.844638
282  283.0  6670.751208  869.366066  0.359764  2.073399  0.846512
283  284.0  6672.345271  863.107847  0.362199  2.098157  0.843610
284  285.0  6671.270708  868.499813  0.359620  2.102595  0.844591
285  286.0  6670.736833  854.904399  0.363183  2.113914  0.847149
286  287.0  6672.862479  850.514427  0.363623  2.126633  0.842941
287  288.0  6673.817458  855.198366  0.361616  2.137724  0.843186
288  289.0  6675.157812  899.259379  0.356716  2.151651  0.843725
289  290.0  6674.771542  843.668876  0.363758  2.117755  0.843795
290  291.0  6675.670500  850.624581  0.363394  2.174459  0.843368
291  292.0  6670.059604  869.954816  0.361627  2.134483  0.843933
292  293.0  6673.129750  871.572215  0.359750  2.114055  0.845317
293  294.0  6684.176438  877.790686  0.360417  2.145494  0.844547
294  295.0  6669.354250  882.878518  0.358607  2.165440  0.837530
295  296.0  6673.586646  858.408370  0.362924  2.160825  0.845195
296  297.0  6675.506604  867.952435  0.360609  2.137251  0.844361
297  298.0  6676.618479  851.352633  0.362834  2.170461  0.842074
298  299.0  6665.216667  894.840609  0.358950  2.199059  0.842885
299  300.0  6671.994396  847.069712  0.365517  2.169840  0.844708
300  301.0  6674.334771  842.314117  0.364135  2.163851  0.844489

[301 rows x 6 columns]
Latent 0.3641347408294678   0.3607977022727331
BCE 2.1638509114583333 2.1689234269989863
Saved model at epoch 301
